<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[搭建5节点hadoop+zookeeper+hbase+spark+kafka+storm(5):Kafka]]></title>
      <url>%2F2017%2F06%2F26%2Fhadoop5%2F</url>
      <content type="text"><![CDATA[本篇介绍Kafka集群安装。版本：java 1.8，Hadoop 2.7.3，HBase 1.2.5，Zookeeper 3.4.10，Spark 2.1.1，Scala 2.12.2，Kafka 0.10.2.1，Storm 1.1.0。以下操作都是以root用户进行，如需用户组和权限设定需自行配置。 1. 服务器信息及节点分配服务器信息： No Hostname 内网IP 外网IP OS 1 node01 192.168.5.11 192.168.205.50 Centos7 2 node02 192.168.5.12 192.168.205.62 Centos7 3 node03 192.168.5.13 192.168.205.63 Centos7 4 node04 192.168.5.14 192.168.205.70 Centos7 5 node05 192.168.5.15 192.168.205.102 Centos7 节点分配： 节点 node01 node02 node03 node04 node05 namenode YES YES NO NO NO datanode NO NO YES YES YES journalnode YES YES YES YES YES zookeeper YES YES YES YES YES hbase 主Master 备份Master RegionServer RegionServer RegionServer spark YES YES YES YES YES kafka YES YES YES YES YES storm YES YES YES YES YES 2. 安装Kafka2.1 下载安装包1wget http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/0.10.2.1/kafka_2.12-0.10.2.1.tgz 2.2 创建模板配置文件需要修改的配置文件为server.properties。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=1# Switch to enable topic deletion or not, default value is false#delete.topic.enable=true############################# Socket Server Settings ############################## The address the socket server listens on. It will get the value returned from# java.net.InetAddress.getCanonicalHostName() if not configured.# FORMAT:# listeners = listener_name://host_name:port# EXAMPLE:# listeners = PLAINTEXT://your.host.name:9092listeners=PLAINTEXT://node01:9092# Hostname and port the broker will advertise to producers and consumers. If not set,# it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value# returned from java.net.InetAddress.getCanonicalHostName().#advertised.listeners=PLAINTEXT://node01:9092# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL# The number of threads handling network requestsnum.network.threads=3# The number of threads doing disk I/Onum.io.threads=8# The send buffer (SO_SNDBUF) used by the socket serversocket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket serversocket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)socket.request.max.bytes=104857600############################# Log Basics ############################## A comma seperated list of directories under which to store log fileslog.dirs=/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.# This value is recommended to be increased for installations with data dirs located in RAID array.num.recovery.threads.per.data.dir=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync# the OS cache lazily. The following configurations control the flush of data to disk.# There are a few important trade-offs here:# 1. Durability: Unflushed data may be lost if you are not using replication.# 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.# 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.# The settings below allow one to configure the flush policy to flush data after a period of time or# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disk#log.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flush#log.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can# be set to delete segments after a period of time, or after a given size has accumulated.# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens# from the end of the log.# The minimum age of a log file to be eligible for deletion due to agelog.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining# segments don&apos;t drop below log.retention.bytes. Functions independently of log.retention.hours.#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according# to the retention policieslog.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=node01:2181,node02:2181,node03:2181,node04:2181,node05:2181# Timeout in ms for connecting to zookeeperzookeeper.connection.timeout.ms=6000其中： broker.id：broker的id，必需是一个全局(集群)唯一的整数值，即集群中每个kafka server的配置不能相同。此处按node01-1/node02-2/node03-3/node04-4/node05-5来配置。 listeners：socket监听的地址，格式为listeners = security_protocol://host_name:port，端口默认为9092。 log.dirs：日志保存目录 zookeeper.connect：zookeeper连接地址 2.3 安装部署写一个简易部署脚本：kafka.sh123456789#/bin/bashtar -xvf ./kafka_2.12-0.10.2.1.tgzmv ./kafka_2.12-0.10.2.1 /usr/kafkacp -rf ./server.properties /usr/kafka/config/server.propertiesecho &quot;export KAFKA_HOME=/usr/kafka&quot; &gt;&gt; /root/.bash_profileecho &quot;export PATH=\$KAFKA_HOME/bin:\$PATH&quot; &gt;&gt; /root/.bash_profile修改权限并执行，执行时确保kafka_2.12-0.10.2.1.tgz，server.properties，kafka.sh在同一目录下：123chmod +x ./kafka.sh./kafka.shsource /root/.bash_profile 2.4 部署所有节点使用sftp将kafka_2.12-0.10.2.1.tgz，server.properties，kafka.sh传到其他所有节点上并按照以上步骤进行部署，注意根据具体节点修改server.properties中的broker.id和listeners。 3. Kafka使用3.1 启动Kafka集群在所有节点上执行命令：1kafka-server-start.sh -daemon /usr/kafka/config/server.properties 3.2 创建Topic1kafka-topics.sh --zookeeper node01:2181 --create --replication-factor 1 --partitions 1 --topic test 3.3 查看Topic1kafka-topics.sh --zookeeper node01:2181 --list 3.4 创建Producer1kafka-console-producer.sh --broker-list node01:9092 --topic test 3.5 创建Consumer1kafka-console-consumer.sh --bootstrap-server node01:9092 --topic test --from-beginning 3.6 删除Topic1kafka-topics.sh --zookeeper node01:2181 --delete --topic test 参照资料：Kafka0.10.2.0分布式集群安装Kafka学习总结(五)——Kafka集群搭建Kafka【第一篇】Kafka集群搭建Kafka学习(三)：Kafka的内部机制深入(持久化，分布式，通讯协议)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建5节点hadoop+zookeeper+hbase+spark+kafka+storm(4):Spark]]></title>
      <url>%2F2017%2F06%2F23%2Fhadoop4%2F</url>
      <content type="text"><![CDATA[本篇介绍Spark集群安装。版本：java 1.8，Hadoop 2.7.3，HBase 1.2.5，Zookeeper 3.4.10，Spark 2.1.1，Scala 2.12.2，Kafka 0.10.2.1，Storm 1.1.0。以下操作都是以root用户进行，如需用户组和权限设定需自行配置。 1. 服务器信息及节点分配服务器信息： No Hostname 内网IP 外网IP OS 1 node01 192.168.5.11 192.168.205.50 Centos7 2 node02 192.168.5.12 192.168.205.62 Centos7 3 node03 192.168.5.13 192.168.205.63 Centos7 4 node04 192.168.5.14 192.168.205.70 Centos7 5 node05 192.168.5.15 192.168.205.102 Centos7 节点分配： 节点 node01 node02 node03 node04 node05 namenode YES YES NO NO NO datanode NO NO YES YES YES journalnode YES YES YES YES YES zookeeper YES YES YES YES YES hbase 主Master 备份Master RegionServer RegionServer RegionServer spark YES YES YES YES YES kafka YES YES YES YES YES storm YES YES YES YES YES 2. 安装Scala2.1 下载安装包1wget https://downloads.lightbend.com/scala/2.12.2/scala-2.12.2.tgz 2.2 安装写一个简易部署脚本：scala.sh1234567#/bin/bashtar -xvf ./scala-2.12.2.tgzmv ./scala-2.12.2 /usr/scalaecho &quot;export SCALA_HOME=/usr/scala&quot; &gt;&gt; /root/.bash_profileecho &quot;export PATH=\$SCALA_HOME/bin:\$PATH&quot; &gt;&gt; /root/.bash_profile修改权限并执行，执行时确保scala-2.12.2.tgz，scala.sh在同一目录下：123chmod +x ./scala.sh./scala.shsource /root/.bash_profile 2.3 安装所有节点使用sftp将scala-2.12.2.tgz，scala.sh传到其他所有节点上并按照以上步骤进行部署。 3. 安装Spark3.1 下载安装包1wget http://mirror.bit.edu.cn/apache/spark/spark-2.1.1/spark-2.1.1-bin-hadoop2.7.tgz 3.2 创建模板配置文件需要修改的配置文件为spark-env.sh，slaves。 3.2.1 spark-env.sh添加以下环境变量：12345678910export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-3.b12.el7_3.x86_64/jreexport SCALA_HOME=/usr/scalaexport HADOOP_HOME=/usr/hadoopexport HADOOP_CONF_DIR=/usr/hadoop/etc/hadoopexport SPARK_DAEMON_Java_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181,node04:2181,node05:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;export SPARK_EXECUTOR_MEMORY=1Gexport SPARK_WORKER_CORES=1export SPARK_WORKER_INSTANCES=1export SPARK_WORKER_MEMORY=1Gexport SPARK_HOME=/usr/spark 3.2.1 slaves12345node01node02node03node04node05 3.3 安装部署写一个简易部署脚本：spark.sh12345678910#/bin/bashtar -xvf spark-2.1.1-bin-hadoop2.7.tgzmv ./spark-2.1.1-bin-hadoop2.7 /usr/sparkcp -rf ./spark-env.sh /usr/spark/conf/spark-env.shcp -rf ./slaves /usr/spark/conf/slavesecho &quot;export SPARK_HOME=/usr/spark&quot; &gt;&gt; /root/.bash_profileecho &quot;export PATH=\$SPARK_HOME/bin:\$PATH&quot; &gt;&gt; /root/.bash_profile修改权限并执行，执行时确保spark-env.sh，slaves，spark-2.1.1-bin-hadoop2.7.tgz，spark.sh在同一目录下：123chmod +x ./spark.sh./spark.shsource /root/.bash_profile 3.4 部署所有节点使用sftp将spark-env.sh，slaves，spark-2.1.1-bin-hadoop2.7.tgz，spark.sh传到其他所有节点上并按照以上步骤进行部署。 4. Spark启动node02作为备用master，基于zookeeper来自动选举active机器。 4.1 在node01执行以下命令1/usr/spark/sbin/start-all.sh 4.2 在node02执行以下命令1/usr/spark/sbin/start-master.sh 5. 测试验证5.1 通过浏览器查看Spark信息打开http://192.168.205.50:8080/： 5.2 通过jps查看进程 5.3 运行Spark提供的计算圆周率的示例程序进入node01的/usr/spark目录，基于local master运行：1[root@node01 spark]# ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local ./examples/jars/spark-examples_2.11-2.1.1.jar基于spark on yarn运行：1[root@node01 spark]# ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.11-2.1.1.jar 参照资料：Spark2.1.0分布式集群安装Linux安装Spark集群(CentOS7+Spark2.1.1+Hadoop2.8.0)Spark2.1.1中用各种模式运行计算圆周率的官方DemoHadoop2.7.3+Spark2.1.0完全分布式集群搭建过程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建5节点hadoop+zookeeper+hbase+spark+kafka+storm(3):Hbase]]></title>
      <url>%2F2017%2F06%2F21%2Fhadoop3%2F</url>
      <content type="text"><![CDATA[本篇介绍Hbase集群安装。版本：java 1.8，Hadoop 2.7.3，HBase 1.2.5，Zookeeper 3.4.10，Spark 2.1.1，Scala 2.12.2，Kafka 0.10.2.1，Storm 1.1.0。以下操作都是以root用户进行，如需用户组和权限设定需自行配置。 1. 服务器信息及节点分配服务器信息： No Hostname 内网IP 外网IP OS 1 node01 192.168.5.11 192.168.205.50 Centos7 2 node02 192.168.5.12 192.168.205.62 Centos7 3 node03 192.168.5.13 192.168.205.63 Centos7 4 node04 192.168.5.14 192.168.205.70 Centos7 5 node05 192.168.5.15 192.168.205.102 Centos7 节点分配： 节点 node01 node02 node03 node04 node05 namenode YES YES NO NO NO datanode NO NO YES YES YES journalnode YES YES YES YES YES zookeeper YES YES YES YES YES hbase 主Master 备份Master RegionServer RegionServer RegionServer spark YES YES YES YES YES kafka YES YES YES YES YES storm YES YES YES YES YES 2. 安装Hbase2.1 下载安装包1wget http://mirror.bit.edu.cn/apache/hbase/1.2.6/hbase-1.2.6-bin.tar.gz 2.2 创建模板配置文件需要修改的配置文件为hbase-env.sh，hbase-site.xml，regionservers。 2.2.1 hbase-site.xml采用双HMaster配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ctns/hbase&lt;/value&gt;&lt;!--这里必须跟core-site.xml中的fs.defaultFS键配置一样--&gt; &lt;/property&gt; &lt;!-- 开启分布式模式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--只配置端口，为了配置多个HMaster --&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hbase/tmp&lt;/value&gt; &lt;/property&gt; &lt;!--这里设置hbase API客户端侧缓存值，大于此值就进行一次提交，/opt/hbase-1.2.1/conf/hbase-site.xml统一配置为5M，对所有HTable都生效，那么客户端API就可不设置--&gt; &lt;!--htable.setWriteBufferSize(5242880);//5M --&gt; &lt;property&gt; &lt;name&gt;hbase.client.write.buffer&lt;/name&gt; &lt;value&gt;5242880&lt;/value&gt; &lt;/property&gt; &lt;!--这里设置Master并发最大线程数，经常有人设为300左右--&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.handler.count&lt;/name&gt; &lt;value&gt;20&lt;/value&gt; &lt;/property&gt; &lt;!-- 默认值 ：256M 说明 ：在当前ReigonServer上单个Reigon的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的region。 数据表创建时会预分区，每个预分区最大大小这里设置为10G，防止频繁的split阻塞数据读写， 只有当预分区超过10G时才会进行split，正式环境应该首先预测数据存储时间内的大致数据量， 然后如果每个预分区为10G，计算出分区数，建表时指定分区设置，防止后期频繁split 写法，如果你想设为128兆，但绝不可以写成128M这样，最安全的写法是128*1024*1024的数值，如下 --&gt; &lt;property&gt; &lt;name&gt;hbase.hregion.max.filesize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt; &lt;!-- hbase本质上可以说是HADOOP HDFS的客户端，虽然Hadoop的core-site.xml里设置了文件副本数，但是仍然是客户端传值优先，这里设置为2， 意思是一个文件，最终在Hadoop上总个数为2，正式环境最好设置为3，目前发现此值小于3时， 在遇到All datanodes xxx.xxx.xxx.xxx:port are bad. Aborting...错误信息时，如果某个DataNode宕机，原则上hbase调用的DFSClient会去其他Datanode 上重试写，但发现配置的值低于3就不会去尝试 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!-- IncreasingToUpperBoundRegionSplitPolicy策略的意思是，数据表如果预分区为2，配置的memstore flush size=128M，那么下一次分裂大小是2的平方然后乘以128MB，即2*2*128M=512MB; ConstantSizeRegionSplitPolicy策略的意思是按照上面指定的region大小超过30G才做分裂 --&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.region.split.policy&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy&lt;/value&gt; &lt;/property&gt; &lt;!--一个edit版本在内存中的cache时长，默认3600000毫秒--&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.optionalcacheflushinterval&lt;/name&gt; &lt;value&gt;7200000&lt;/value&gt; &lt;/property&gt; &lt;!--分配给HFile/StoreFile的block cache占最大堆(-Xmx setting)的比例。默认0.3意思是分配30%，设置为0就是禁用，但不推荐。--&gt; &lt;property&gt; &lt;name&gt;hfile.block.cache.size&lt;/name&gt; &lt;value&gt;0.3&lt;/value&gt; &lt;/property&gt; &lt;!--当memstore的大小超过这个值的时候，会flush到磁盘。这个值被一个线程每隔hbase.server.thread.wakefrequency检查一下。--&gt; &lt;property&gt; &lt;name&gt;hbase.hregion.memstore.flush.size&lt;/name&gt; &lt;value&gt;52428800&lt;/value&gt; &lt;/property&gt; &lt;!-- 默认值 ：0.4/0.35 说明 ：hbase.hregion.memstore.flush.size 这个参数的作用是当单个Region内所有的memstore大小总和超过指定值时，flush该region的所有memstore 单个region server的全部memtores的最大值。超过这个值，一个新的update操作会被挂起，强制执行flush操作。 以前版本中是通过hbase.regionserver.global.memstore.upperLimit设置，老版本中含义是在hbase-env.sh中配置的HEAP_SIZE比如4G， 那么以该值4G乘以配置的0.5就是2G，意思是所有memstore总和达到2G值时，阻塞所有读写，现在1.2.1版本hbase中被hbase.regionserver.global.memstore.size替代， 计算方法仍然是HEAP_SIZE乘以配置的百分比比如下面的0.5，那么阻塞读写的阀值就为2G --&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.global.memstore.size&lt;/name&gt; &lt;value&gt;0.5&lt;/value&gt; &lt;/property&gt; &lt;!-- 当强制执行flush操作的时候，当低于这个值的时候，flush会停止。 默认是堆大小的 35% . 如果这个值和 hbase.regionserver.global.memstore.upperLimit 相同就意味着当update操作因为内存限制被挂起时， 会尽量少的执行flush(译者注:一旦执行flush，值就会比下限要低，不再执行)。 在老版本中该值是通过hbase.regionserver.global.memstore.size.lower.limit设置， 计算方法是以hbase-env.sh的HEAP_SIZE乘以配置的百分比比如0.3就是HEAP_SIZE4G乘以0.3=1.2G，达到这个值的话就在所有memstore中选择最大的那个做flush动作， 新版本则完全不同了，首先是通过hbase.regionserver.global.memstore.lowerLimit设置，而且不是以HEAP_SIZE作为参考， 二是以配置的hbase.regionserver.global.memstore.size的值再乘以配置的比例比如0.5，如果HEAP_SIZE=4G， hbase.regionserver.global.memstore.size配置为0.5，hbase.regionserver.global.memstore.size.lower.limit配置的为0.5， 则计算出来的值为4G乘以0.5再乘以0.5就是1G了，达到1G就先找最大的memstore触发flush --&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.global.memstore.size.lower.limit&lt;/name&gt; &lt;value&gt;0.5&lt;/value&gt; &lt;/property&gt; &lt;!--这里设置HDFS客户端最大超时时间，尽量改大，后期hbase经常会因为该问题频繁宕机--&gt; &lt;property&gt; &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt; &lt;value&gt;600000&lt;/value&gt; &lt;/property&gt; &lt;!-- hbase.table.sanity.checks是一个开关，主要用于hbase各种参数检查，当为true时候，检查步骤如下 1.check max file size，hbase.hregion.max.filesize，最小为2MB 2.check flush size，hbase.hregion.memstore.flush.size，最小为1MB 3.check that coprocessors and other specified plugin classes can be loaded 4.check compression can be loaded 5.check encryption can be loaded 6.Verify compaction policy 7.check that we have at least 1 CF 8.check blockSize 9.check versions 10.check minVersions &lt;= maxVerions 11.check replication scope 12.check data replication factor, it can be 0(default value) when user has not explicitly set the value, in this case we use default replication factor set in the file system. 详细情况可以去查看源代码org.apache.hadoop.hbase.master.HMaster的方法sanityCheckTableDescriptor， 该代码位于hbase源码的模块hbase-server下 --&gt; &lt;property&gt; &lt;name&gt;hbase.table.sanity.checks&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!--ZooKeeper 会话超时.HBase把这个值传递改zk集群，向他推荐一个会话的最大超时时间--&gt; &lt;property&gt; &lt;!--every 30s,the master will check regionser is working --&gt; &lt;name&gt;zookeeper.session.timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!-- Hbase的外置zk集群时，使用下面的zk端口。因为我这5台机子打算都安装hbase，所以都指定zookeeper。 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181,node04:2181,node05:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/usr/zookeeper/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.2.2 hbase-env.sh添加如下变量：123export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-3.b12.el7_3.x86_64/jreexport HBASE_CLASSPATH=/usr/hadoop/etc/hadoopexport HBASE_MANAGES_ZK=false 2.2.3 regionservers123node03node04node05 2.3 安装部署部署时需要替换hbase的lib中版本不一致的hadoop相关jar包，如果使用了hadoop HA别名的话需要在hbase/conf下建立hdfs-site.xml和core-site.xml的软连接，写一个简易部署脚本：hbase.sh1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#/bin/bashtar -xvf ./hbase-1.2.6-bin.tar.gzmv ./hbase-1.2.6 /usr/hbasemv /usr/hbase/lib/hadoop-annotations-2.5.1.jar /usr/hbase/lib/hadoop-annotations-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-auth-2.5.1.jar /usr/hbase/lib/hadoop-auth-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-common-2.5.1.jar /usr/hbase/lib/hadoop-common-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-client-2.5.1.jar /usr/hbase/lib/hadoop-client-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-hdfs-2.5.1.jar /usr/hbase/lib/hadoop-hdfs-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar /usr/hbase/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-mapreduce-client-app-2.5.1.jar /usr/hbase/lib/hadoop-mapreduce-client-app-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar /usr/hbase/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-mapreduce-client-common-2.5.1.jar /usr/hbase/lib/hadoop-mapreduce-client-common-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-mapreduce-client-core-2.5.1.jar /usr/hbase/lib/hadoop-mapreduce-client-core-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-yarn-server-common-2.5.1.jar /usr/hbase/lib/hadoop-yarn-server-common-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-yarn-common-2.5.1.jar /usr/hbase/lib/hadoop-yarn-common-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-yarn-api-2.5.1.jar /usr/hbase/lib/hadoop-yarn-api-2.5.1.jar.bakmv /usr/hbase/lib/hadoop-yarn-client-2.5.1.jar /usr/hbase/lib/hadoop-yarn-client-2.5.1.jar.bakcp /usr/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar /usr/hbase/lib/hadoop-auth-2.7.3.jarcp /usr/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar /usr/hbase/lib/hadoop-annotations-2.7.3.jarcp /usr/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar /usr/hbase/lib/hadoop-common-2.7.3.jarcp /usr/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar /usr/hbase/lib/hadoop-hdfs-2.7.3.jarcp /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar /usr/hbase/lib/hadoop-mapreduce-client-jobclient-2.7.3.jarcp /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar /usr/hbase/lib/hadoop-mapreduce-client-app-2.7.3.jarcp /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar /usr/hbase/lib/hadoop-mapreduce-client-shuffle-2.7.3.jarcp /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar /usr/hbase/lib/hadoop-mapreduce-client-common-2.7.3.jarcp /usr/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar /usr/hbase/lib/hadoop-mapreduce-client-core-2.7.3.jarcp /usr/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar /usr/hbase/lib/hadoop-yarn-server-common-2.7.3.jarcp /usr/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar /usr/hbase/lib/hadoop-yarn-common-2.7.3.jarcp /usr/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar /usr/hbase/lib/hadoop-yarn-api-2.7.3.jarcp /usr/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar /usr/hbase/lib/hadoop-yarn-client-2.7.3.jar#移除HBase里面的不必要log4j的jar包mv /usr/hbase/lib/slf4j-log4j12-1.7.5.jar /usr/hbase/lib/slf4j-log4j12-1.7.5.jar.bakcp -rf ./hbase-env.sh /usr/hbase/conf/hbase-env.shcp -rf ./hbase-site.xml /usr/hbase/conf/hbase-site.xmlcp -rf ./regionservers /usr/hbase/conf/regionserversmkdir /usr/hbase/tmp#创建软连接，识别hadoop HA别名ln -s /usr/hadoop/etc/hadoop/hdfs-site.xml /usr/hbase/conf/hdfs-site.xmlln -s /usr/hadoop/etc/hadoop/core-site.xml /usr/hbase/conf/core-site.xmlecho &quot;export HBASE_HOME=/usr/hbase&quot; &gt;&gt; /root/.bash_profileecho &quot;export PATH=\$HBASE_HOME/bin:\$PATH&quot; &gt;&gt; /root/.bash_profile修改权限并执行，执行时确保hbase-env.sh，hbase-site.xml，regionservers，hbase-1.2.6-bin.tar.gz，hbase.sh在同一目录下：123chmod +x ./hbase.sh./hbase.shsource /root/.bash_profile 2.4 部署所有节点使用sftp将hbase-env.sh，hbase-site.xml，regionservers，hbase-1.2.6-bin.tar.gz，hbase.sh传到其他所有节点上并按照以上步骤进行部署。 3. Hbase启动在启动之前将ZooKeeper和Hadoop启动。 3.1 在node01执行以下命令1start-hbase.sh 3.2 在node02执行以下命令1hbase-daemon.sh start master 3.3 查看相应进程在node01和node02上用jps查看是否有HMaster进程，在node03, node04和node05上用jps查看是否有HRegionServer进程。 4. 测试验证4.1 节点状态查看hbase状态http://192.168.205.50:16010/： 4.2 hbase shell测试4.2.1 进入hbase shell命令行：1hbase shell 4.2.2 建立一个表,具有三个列族member_id 、address、info：1create &apos;member&apos;,&apos;member_id&apos;,&apos;address&apos;,&apos;info&apos; 4.2.3 查看当前HBase中具有哪些表：1list 4.2.4 查看表的构造：1describe &apos;member&apos; 4.2.5 删除member表：1disable &apos;member&apos; 1drop &apos;member&apos; 4.2.6 退出shell：1exit 参照资料：Hbase HA 分布式搭建在hadoop2.8的HA+zookeeper的环境下安装分布式HBase 1.2.5HBase安装配置之完全分布式模式]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建5节点hadoop+zookeeper+hbase+spark+kafka+storm(2):Hadoop]]></title>
      <url>%2F2017%2F06%2F14%2Fhadoop2%2F</url>
      <content type="text"><![CDATA[本篇介绍Hadoop集群安装。版本：java 1.8，Hadoop 2.7.3，HBase 1.2.5，Zookeeper 3.4.10，Spark 2.1.1，Scala 2.12.2，Kafka 0.10.2.1，Storm 1.1.0。以下操作都是以root用户进行，如需用户组和权限设定需自行配置。 1. 概述hadoop2中NameNode可以有多个（目前只支持2个）。每一个都有相同的职能。一个是active状态的，一个是standby状态的。当集群运行时，只有active状态的NameNode是正常工作的，standby状态的NameNode是处于待命状态的，时刻同步active状态NameNode的数据。一旦active状态的NameNode不能工作，standby状态的NameNode就可以转变为active状态的，就可以继续工作了。2个NameNode的数据其实是实时共享的。新HDFS采用了一种共享机制，Quorum Journal Node（JournalNode）集群或者Nnetwork File System（NFS）进行共享。NFS是操作系统层面的，JournalNode是hadoop层面的，我们这里使用JournalNode集群进行数据共享（这也是主流的做法）。JournalNode的架构图如下：两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了。对于HA集群而言，确保同一时刻只有一个NameNode处于active状态是至关重要的。否则，两个NameNode的数据状态就会产生分歧，可能丢失数据，或者产生错误的结果。为了保证这点，这就需要利用使用ZooKeeper了。首先HDFS集群中的两个NameNode都在ZooKeeper中注册，当active状态的NameNode出故障时，ZooKeeper能检测到这种情况，它就会自动把standby状态的NameNode切换为active状态。 2. 服务器信息及节点分配服务器信息： No Hostname 内网IP 外网IP OS 1 node01 192.168.5.11 192.168.205.50 Centos7 2 node02 192.168.5.12 192.168.205.62 Centos7 3 node03 192.168.5.13 192.168.205.63 Centos7 4 node04 192.168.5.14 192.168.205.70 Centos7 5 node05 192.168.5.15 192.168.205.102 Centos7 节点分配： 节点 node01 node02 node03 node04 node05 namenode YES YES NO NO NO datanode NO NO YES YES YES journalnode YES YES YES YES YES zookeeper YES YES YES YES YES hbase 主Master 备份Master RegionServer RegionServer RegionServer spark YES YES YES YES YES kafka YES YES YES YES YES storm YES YES YES YES YES 3. 安装Hadoop3.1 下载安装包1wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz 3.2 创建模板配置文件需要修改的配置文件为core-site.xml，hadoop-env.sh，hdfs-site.xml，mapred-site.xml，slaves，yarn-env.sh，yarn-site.xml。 3.2.1 core-site.xml注意hadoop集群中的主机名不能带有下划线。12345678910111213141516171819202122&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ctns--&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ctns&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop数据临时存放目录--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper地址--&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181,node04:2181,node05:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.2.2 hdfs-site.xml采用双namenode的HA配置：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ctns，需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ctns&lt;/value&gt; &lt;/property&gt; &lt;!-- ctns下面有两个NameNode，分别是node01，node02 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ctns&lt;/name&gt; &lt;value&gt;node01, node02&lt;/value&gt; &lt;/property&gt; &lt;!-- node01 的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ctns.node01&lt;/name&gt; &lt;value&gt;node01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- node01 的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ctns.node01&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- node02 的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ctns.node02&lt;/name&gt; &lt;value&gt;node02:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- node02 的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ctns.node02&lt;/name&gt; &lt;value&gt;node02:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node01:8485;node02:8485;node03:8485;node04:8485;node05:8485/ctns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop/journal&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode故障时自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ctns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 当前节点为name节点时的元信息存储路径.这个参数设置为多个目录，那么这些目录下都保存着元信息的多个备份 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///usr/hadoop/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;!-- 当前节点为data节点时的元信息存储路径.这个参数设置为多个目录，那么这些目录下都保存着数据信息的多个备份 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///usr/hadoop/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.2.3 mapred-site.xml123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.2.4 yarn-site.xml采用双resourcemanager的HA配置：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;!-- 超时的周期 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 打开高可用 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 启动故障自动恢复 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-rm-cluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 给yarn cluster 取个名字yarn-rm-cluster --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 给ResourceManager 取个名字 rm1,rm2 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;node01&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置ResourceManager rm1 hostname --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;node02&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置ResourceManager rm2 hostname --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用resourcemanager 自动恢复 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181,node04:2181,node05:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置Zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181,node04:2181,node05:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置Zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;node01:8032&lt;/value&gt; &lt;/property&gt; &lt;!-- rm1端口号 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;node01:8034&lt;/value&gt; &lt;/property&gt; &lt;!-- rm1调度器的端口号 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;node01:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- rm1 webapp端口号 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;node02:8032&lt;/value&gt; &lt;/property&gt; &lt;!-- rm2端口号 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;node02:8034&lt;/value&gt; &lt;/property&gt; &lt;!-- rm2调度器的端口号 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;node02:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- rm2 webapp端口号 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;!-- 执行MapReduce需要配置的shuffle过程 --&gt;&lt;/configuration&gt; 3.2.5 slaves123node03node04node05 3.2.6 hadoop-env.sh &amp; yarn-env.sh添加JAVA_HOME:1export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-3.b12.el7_3.x86_64/jre 3.3 安装部署写一个简易部署脚本：hadoop.sh123456789101112131415# /bin/bashtar -xvf ./hadoop-2.7.3.tar.gzmv ./hadoop-2.7.3 /usr/hadoopmkdir -p /usr/hadoop/hdfs/data /usr/hadoop/hdfs/name /usr/hadoop/tmpcp -rf ./core-site.xml /usr/hadoop/etc/hadoop/core-site.xmlcp -rf ./hadoop-env.sh /usr/hadoop/etc/hadoop/hadoop-env.shcp -rf ./hdfs-site.xml /usr/hadoop/etc/hadoop/hdfs-site.xmlcp -rf ./mapred-site.xml /usr/hadoop/etc/hadoop/mapred-site.xmlcp -rf ./slaves /usr/hadoop/etc/hadoop/slavescp -rf ./yarn-env.sh /usr/hadoop/etc/hadoop/yarn-env.shcp -rf ./yarn-site.xml /usr/hadoop/etc/hadoop/yarn-site.xmlecho &quot;export HADOOP_HOME=/usr/hadoop&quot; &gt;&gt; /root/.bash_profileecho &quot;export PATH=\$HADOOP_HOME/bin:\$PATH&quot; &gt;&gt; /root/.bash_profileecho &quot;export PATH=\$HADOOP_HOME/sbin:\$PATH&quot; &gt;&gt; /root/.bash_profile修改权限并执行，执行时确保core-site.xml，hadoop-2.7.3.tar.gz，hadoop-env.sh，hadoop.sh，hdfs-site.xml，mapred-site.xml，slaves，yarn-env.sh，yarn-site.xml在同一目录下：123chmod +x ./hadoop.sh./hadoop.shsource /root/.bash_profile 3.4 部署所有节点使用sftp将core-site.xml，hadoop-2.7.3.tar.gz，hadoop-env.sh，hadoop.sh，hdfs-site.xml，mapred-site.xml，slaves，yarn-env.sh，yarn-site.xml传到其他所有节点上并按照以上步骤进行部署。 4. Hadoop启动注意首次初始化启动命令和之后启动的命令是不同的，首次启动比较复杂，步骤不对的话就会报错，不过之后就好了。 4.1 首次启动命令4.1.1 如果未启动Zookeeper，首先启动各个节点的Zookeeper，在各个节点上执行以下命令：1zkServer.sh start 4.1.2 在某一个namenode节点执行如下命令，创建命名空间：1hdfs zkfc -formatZK 4.1.3 在每个journalnode节点用如下命令启动journalnode：1hadoop-daemon.sh start journalnode 4.1.4 在主namenode节点用格式化namenode和journalnode目录：1hdfs namenode -format ctns 4.1.5 在主namenode节点启动namenode进程：1hadoop-daemon.sh start namenode 4.1.6 在备namenode节点执行第一行命令，这个是把备namenode节点的目录格式化并把元数据从主namenode节点copy过来，并且这个命令不会把journalnode目录再格式化了，然后用第二个命令启动备namenode进程：12hdfs namenode -bootstrapStandbyhadoop-daemon.sh start namenode 4.1.7 在两个namenode节点都执行以下命令：1hadoop-daemon.sh start zkfc 4.1.8 在所有datanode节点都执行以下命令启动datanode：1hadoop-daemon.sh start datanode 4.1.9 启动resourcemanager和nodemanager：在node01上执行：12yarn-daemon.sh start resourcemanageryarn-daemons.sh start nodemanager 4.2 日常启停命令12./start-all.sh./stop-all.sh 5. 测试验证5.1 节点状态查看namenode状态http://192.168.205.50:50070/：http://192.168.205.62:50070/：查看resourcemanager状态http://192.168.205.62:8088/： 5.2 mapreduce创建input目录：1hadoop fs -mkdir -p /hadoop/input使用hadoop的根目录下自带的LICENSE.txt文件测试:1hadoop fs -put /usr/hadoop/LICENSE.txt /hadoop/input使用wordcount测试类进行处理测试:123cd /usr/hadoop/share/hadoop/mapreduce/ls --查看hadoop-mapreduce-examples-2.7.3.jarhadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /hadoop/input /hadoop/output稍后会在/hadoop/output路径下看到结果:1hadoop fs -ls /hadoop/output 参照资料：hadoop、zookeeper、hbase、spark集群环境搭建Hadoop双namenode配置搭建（HA）Hadoop的namenode 从单点向双namenode的HA的升级过程，含wordcount验证搭建5个节点的hadoop集群环境（CDH5）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建5节点hadoop+zookeeper+hbase+spark+kafka+storm(1):Zookeeper]]></title>
      <url>%2F2017%2F06%2F12%2Fhadoop1%2F</url>
      <content type="text"><![CDATA[本篇介绍Zookeeper集群安装。版本：java 1.8，Hadoop 2.7.3，HBase 1.2.5，Zookeeper 3.4.10，Spark 2.1.1，Scala 2.12.2，Kafka 0.10.2.1，Storm 1.1.0。以下操作都是以root用户进行，如需用户组和权限设定需自行配置。 1. 服务器信息及节点分配服务器信息： No Hostname 内网IP 外网IP OS 1 node01 192.168.5.11 192.168.205.50 Centos7 2 node02 192.168.5.12 192.168.205.62 Centos7 3 node03 192.168.5.13 192.168.205.63 Centos7 4 node04 192.168.5.14 192.168.205.70 Centos7 5 node05 192.168.5.15 192.168.205.102 Centos7 节点分配： 节点 node01 node02 node03 node04 node05 namenode YES YES NO NO NO datanode NO NO YES YES YES journalnode YES YES YES YES YES zookeeper YES YES YES YES YES hbase 主Master 备份Master RegionServer RegionServer RegionServer spark YES YES YES YES YES kafka YES YES YES YES YES storm YES YES YES YES YES 2. 安装Zookeeper2.1 下载安装包1wget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz 2.2 创建zoo.cfg复制zoo_sample.cfg为zoo.cfg，修改dataDir和dataLogDir路径，并在最后添加节点信息：zoo.cfg1234567891011121314151617181920212223242526272829303132333435# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/usr/zookeeper/datadataLogDir=/usr/zookeeper/logs# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1server.1=node01:2888:3888server.2=node02:2888:3888server.3=node03:2888:3888server.4=node04:2888:3888server.5=node05:2888:3888 2.3 安装部署写一个简易部署脚本：zk.sh123456789# /bin/bashtar -xvf ./zookeeper-3.4.10.tar.gzmv ./zookeeper-3.4.10 /usr/zookeepermkdir /usr/zookeeper/datamkdir /usr/zookeeper/logsmv ./zoo.cfg /usr/zookeeper/conf/zoo.cfgecho &quot;export ZOOKEEPER_HOME=/usr/zookeeper&quot; &gt;&gt; /root/.bash_profileecho &quot;export PATH=\$ZOOKEEPER_HOME/bin:\$PATH&quot; &gt;&gt; /root/.bash_profile修改权限并执行，执行时确保zk.sh, zoo.cfg和zookeeper-3.4.10.tar.gz在同一目录下：123chmod +x ./zk.sh./zk.shsource /root/.bash_profile 2.4 创建myid在zookeeper的dataDir下添加myid，记录该节点的zk id，注意id要与zoo.cfg中该节点的server.x匹配，以server.1为例：12cd /usr/zookeeper/dataecho &quot;1&quot; &gt; myid 2.5 部署所有节点使用sftp将zk.sh, zoo.cfg, zookeeper-3.4.10.tar.gz传到其他所有节点上并按照以上步骤进行部署。 3. Zookeeper使用3.1 启动Zookeeper集群所有节点上执行：1zkServer.sh start 3.2 查看Zookeeper状态123456789101112131415161718192021222324[root@node01 ~]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/bin/../conf/zoo.cfgMode: follower[root@node01 ~]# ssh node02[root@node02 ~]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/bin/../conf/zoo.cfgMode: follower[root@node02 ~]# ssh node03[root@node03 ~]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/bin/../conf/zoo.cfgMode: follower[root@node03 ~]# ssh node04[root@node04 ~]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/bin/../conf/zoo.cfgMode: follower[root@node04 ~]# ssh node05[root@node05 ~]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/zookeeper/bin/../conf/zoo.cfgMode: leader 3.3 停止Zookeeper1zkServer.sh stop 3.4 重启Zookeeper1zkServer.sh restart 参照资料：hadoop、zookeeper、hbase、spark集群环境搭建Zookeeper3.4.9分布式集群安装]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建5节点hadoop+zookeeper+hbase+spark+kafka+storm(0):准备]]></title>
      <url>%2F2017%2F06%2F12%2Fhadoop0%2F</url>
      <content type="text"><![CDATA[本篇介绍安装前的准备工作。版本：java 1.8，Hadoop 2.7.3，HBase 1.2.5，Zookeeper 3.4.10，Spark 2.1.1，Scala 2.12.2，Kafka 0.10.2.1，Storm 1.1.0。以下操作都是以root用户进行，如需用户组和权限设定需自行配置。 1. 服务器信息及节点分配服务器信息： No Hostname 内网IP 外网IP OS 1 node01 192.168.5.11 192.168.205.50 Centos7 2 node02 192.168.5.12 192.168.205.62 Centos7 3 node03 192.168.5.13 192.168.205.63 Centos7 4 node04 192.168.5.14 192.168.205.70 Centos7 5 node05 192.168.5.15 192.168.205.102 Centos7 节点分配： 节点 node01 node02 node03 node04 node05 namenode YES YES NO NO NO datanode NO NO YES YES YES journalnode YES YES YES YES YES zookeeper YES YES YES YES YES hbase 主Master 备份Master RegionServer RegionServer RegionServer spark YES YES YES YES YES kafka YES YES YES YES YES storm YES YES YES YES YES 2. 修改hosts文件和主机名分别修改5个服务器的hosts文件和主机名。 2.1 修改hosts文件编辑hosts文件：1sudo vi /etc/hosts添加下面内容，注意使用内网ip，浮动ip可能导致端口无法绑定：12345192.168.5.11 node01192.168.5.12 node02192.168.5.13 node03192.168.5.14 node04192.168.5.15 node05 2.2 修改主机名使用hostname修改主机名为node01/node02/node03/node04/node05。注意主机名和hosts文件中设置的名称应当保持一致，否则会产生意外的错误。 3. 关闭防火墙3.1 关闭命令12service iptables stopchkconfig iptables off 3.2 查看防火墙状态1service iptables status 4. 设置NTP时间同步为了保证集群内的时间是一致的，我们可以将其中的一台主机作为时间服务器，其他主机设置定时任务每天与时间服务器同步一次时间。 4.1 配置某台主机为ntp时间服务器4.1.1 安装ntp1yum -y install ntp 4.1.2 修改配置文件/etc/ntp.conf12#允许192.*.*.*的主机进行时间同步restrict 192.0.0.0 mask 255.0.0.0 nomodify notrap 4.1.3 手动同步1ntpdate 0.asia.pool.ntp.org 4.1.4 启动服务1service ntpd start 4.1.5 开机启动1chkconfig ntpd on 4.2 配置其他主机为ntp客户端4.2.1 安装ntp1yum -y install ntp 4.2.2 修改配置文件/etc/ntp.conf123456#注释默认配置，添加刚刚创建的ntpserver#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver 192.168.5.11 4.2.3 编辑定时器任务1sudo crontab -e 添加如下内容：10 0 * * * /usr/sbin/ntpdate 192.168.5.11 &gt;&gt; /usr/hadoop/logs/ntpd.log 5. 配置SSH免登录5.1 生成SSH的公钥所有节点执行以下命令：1ssh-keygen 5.2 设置免登录所有节点执行以下命令：12345ssh-copy-id -i node01ssh-copy-id -i node02ssh-copy-id -i node03ssh-copy-id -i node04ssh-copy-id -i node05 5.3 验证1ssh node02 6. 安装jdk6.1 yum安装openjdk所有节点执行以下命令：12yum install java-1.8.0-openjdk.x86_64yum install java-1.8.0-openjdk-devel.x86_64完成后执行以下命令验证：1234[root@node01 ~]# java -versionopenjdk version &quot;1.8.0_131&quot;OpenJDK Runtime Environment (build 1.8.0_131-b12)OpenJDK 64-Bit Server VM (build 25.131-b12, mixed mode) 6.2 设置环境变量以bash为例，打开/root下的.bash_profile：1vi /root/.bash_profile添加JAVA_HOME：12export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-3.b12.el7_3.x86_64/jreexport PATH=$JAVA_HOME/bin:$PATH执行以下命令生效：1source /root/.bash_profile 至此准备工作基本完成，下篇开始安装组件。 参照资料：hadoop、zookeeper、hbase、spark集群环境搭建]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes一键部署Mycat+Mysql主从集群]]></title>
      <url>%2F2017%2F06%2F01%2Fmysqlcluster%2F</url>
      <content type="text"><![CDATA[Kubernetes一键部署一主一从，读写分离，自动切换的mycat+mysql架构，其中mycat配置文件、mysqsl数据文件的volume挂载未涉及，可根据实际情况进行修改。所有代码参照github k8smysqlcluster。后期进行双主双从、mycat高可用的扩展。 1. Mysql复制原理Mysql内建的复制功能是构建大型，高性能应用程序的基础。将Mysql的数据分布到多个系统上去，这种分布的机制，是通过将Mysql的某一台主机的数据复制到其它主机（slaves）上，并重新执行一遍来实现的。复制过程中一个服务器充当主服务器，而一个或多个其它服务器充当从服务器。主服务器将更新写入二进制日志文件，并维护文件的一个索引以跟踪日志循环。这些日志可以记录发送到从服务器的更新。当一个从服务器连接主服务器时，它通知主服务器从服务器在日志中读取的最后一次成功更新的位置。从服务器接收从那时起发生的任何更新，然后封锁并等待主服务器通知新的更新。请注意当你进行复制时，所有对复制中的表的更新必须在主服务器上进行。否则，你必须要小心，以避免用户对主服务器上的表进行的更新与对从服务器上的表所进行的更新之间的冲突。 1.1 mysql支持的复制类型： 基于语句的复制：在主服务器上执行的SQL语句，在从服务器上执行同样的语句。MySQL默认采用基于语句的复制，效率比较高。一旦发现没法精确复制时， 会自动选着基于行的复制。 基于行的复制：把改变的内容复制过去，而不是把命令在从服务器上执行一遍. 从mysql5.0开始支持 混合类型的复制：默认采用基于语句的复制，一旦发现基于语句的无法精确的复制时，就会采用基于行的复制。 1.2 复制解决的问题MySQL复制技术有以下一些特点： 数据分布 (Data distribution ) 负载平衡(load balancing) 备份(Backups) 高可用性和容错行(High availability and failover) 1.3 复制如何工作整体上来说，复制有3个步骤： master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； slave将master的binary log events拷贝到它的中继日志(relay log)； slave重做中继日志中的事件，将改变反映它自己的数据。 下图描述了复制的过程：该过程的第一部分就是master记录二进制日志。在每个事务更新数据完成之前，master在二日志记录这些改变。MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的。在事件写入二进制日志完成后，master通知存储引擎提交事务。下一步就是slave将master的binary log拷贝到它自己的中继日志。首先，slave开始一个工作线程——I/O线程。I/O线程在master上打开一个普通的连接，然后开始binlog dump process。Binlog dump process从master的二进制日志中读取事件，如果已经跟上master，它会睡眠并等待master产生新的事件。I/O线程将这些事件写入中继日志。SQL slave thread（SQL从线程）处理该过程的最后一步。SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致。只要该线程与I/O线程保持一致，中继日志通常会位于OS的缓存中，所以中继日志的开销很小。此外，在master中也有一个工作线程：和其它MySQL的连接一样，slave在master中打开一个连接也会使得master开始一个线程。复制过程有一个很重要的限制——复制在slave上是串行化的，也就是说master上的并行更新操作不能在slave上并行操作。 2. Mycat介绍从定义和分类来看，它是一个开源的分布式数据库系统，是一个实现了MySQL协议的服务器，前端用户可以把它看作是一个数据库代理，用MySQL客户端工具和命令行访问，而其后端可以用MySQL原生协议与多个MySQL服务器通信，也可以用JDBC协议与大多数主流数据库服务器通信，其核心功能是分表分库，即将一个大表水平分割为N个小表，存储在后端MySQL服务器里或者其他数据库里。 MyCat发展到目前的版本，已经不是一个单纯的MySQL代理了，它的后端可以支持MySQL、SQL Server、Oracle、DB2、PostgreSQL等主流数据库，也支持MongoDB这种新型NoSQL方式的存储，未来还会支持更多类型的存储。而在最终用户看来，无论是那种存储方式，在MyCat里，都是一个传统的数据库表，支持标准的SQL语句进行数据的操作，这样一来，对前端业务系统来说，可以大幅降低开发难度，提升开发速度。 详细内容可参照Mycat官网。Mycat高可用方案： Mycat双主架构： 3. Kubernetes部署基于Mycat的读写分离，自动切换的主从Mysql架构Build出mysql-master和mysql-slave的镜像实现主从配置，build mycat镜像实现读写分离和自动切换配置，基于rc和svc yaml文件使用kubernetes进行快速部署。 3.1 mysql-master基于官方mysql镜像，修改mysqld.cnf添加server-id和binlog，修改docker-entrypoint.sh在mysql上创建同步账号并授权。mysql-master Dockerfile12345FROM mysqlRUN sed -i &apos;/\[mysqld\]/a server-id=1\nlog-bin=mysql-bin&apos; /etc/mysql/mysql.conf.d/mysqld.cnfRUN echo &quot;$(tac /usr/local/bin/docker-entrypoint.sh | sed &quot;28a echo \&apos;FLUSH PRIVILEGES;\&apos; | \&quot;\$\&#123;mysql\[@\]\&#125;\&quot;\necho \&quot;GRANT REPLICATION SLAVE ON *.* TO \&apos;\$MYSQL_REPLICATION_USER\&apos;@\&apos;%\&apos; IDENTIFIED BY \&apos;\$MYSQL_REPLICATION_PASSWORD\&apos;;\&quot; | \&quot;\$\&#123;mysql\[@\]\&#125;\&quot;&quot; | tac)&quot; &gt; /usr/local/bin/docker-entrypoint.sh 执行如下命令本地build mysql-master镜像;1docker build -t mysql-master -f ./Dockerfile . 3.2 mysql-slave基于官方mysql镜像，修改mysqld.cnf添加server-id和binlog，修改docker-entrypoint.sh添加主机信息并开启从机模式。mysql-slave Dockerfile12345FROM mysqlRUN RAND=&quot;$(date +%s | rev | cut -c 1-2)$(echo $&#123;RANDOM&#125;)&quot; &amp;&amp; sed -i &apos;/\[mysqld\]/a server-id=&apos;$RAND&apos;\nlog-bin=mysql-bin&apos; /etc/mysql/mysql.conf.d/mysqld.cnfRUN echo &quot;$(tac /usr/local/bin/docker-entrypoint.sh | sed &quot;28a echo \&quot;START SLAVE;\&quot; | \&quot;\$\&#123;mysql\[@\]\&#125;\&quot;\necho \&quot;CHANGE MASTER TO master_host=\&apos;\$MYSQL_MASTER_SERVICE_HOST\&apos;, master_user=\&apos;\$MYSQL_REPLICATION_USER\&apos;, master_password=\&apos;\$MYSQL_REPLICATION_PASSWORD\&apos;;\&quot; | \&quot;\$\&#123;mysql\[@\]\&#125;\&quot;\necho \&quot;STOP SLAVE;\&quot; | \&quot;\$\&#123;mysql\[@\]\&#125;\&quot;&quot; | tac)&quot; &gt; /usr/local/bin/docker-entrypoint.sh 上面slave的配置中，master_host一项用的是$MYSQL_MASTER_SERVICE_HOST，这个环境变量（enviromnent variable）是由k8s生成的。k8s的service创建后，会自动分配一个cluster ip，这个cluster ip是动态的，我们没法直接使用或硬编码，k8s为了service对容器的可见，生成了一组环境变量，这些环境变量用于记录service name到cluster ip地址的映射关系，这样容器中就可以使用这些变量来使用service。（类似的，Docker中提供了links。） 举例：如果service的名称为foo，则生成的环境变量如下：FOO_SERVICE_HOSTFOO_SERVICE_PORT更多介绍请参考k8s官方资料：http://kubernetes.io/docs/user-guide/container-environment/ 执行如下命令本地build mysql-slave镜像;1docker build -t mysql-slave -f ./Dockerfile . 3.3 mycat基于gaven/mycat镜像，修改schema.xml进行一主一从读写分离自动切换配置，修改server.xml添加db用户名密码等。mycat Dockerfile1234567FROM gaven/mycatCOPY schema.xml /usr/local/mycat/conf/schema.xmlCOPY server.xml /usr/local/mycat/conf/server.xmlCOPY log4j2.xml /usr/local/mycat/conf/log4j2.xmlCOPY entrypoint.sh /root/CMD [&quot;/root/entrypoint.sh&quot;] 配置文件参照github k8smysqlcluster。 执行如下命令本地build mycat镜像;1docker build -t jacob/mycat -f ./Dockerfile . 3.4 kubernetes filesrc和svc文件参照github k8smysqlcluster。 执行如下命令启动，注意顺序为先启动mysql-master，再启动mysql-slave，最后启动mycat;123456kubectl create -f ./rc-mysql-masterkubectl create -f ./svc-mysql-masterkubectl create -f ./rc-mysql-slavekubectl create -f ./svc-mysql-slavekubectl create -f ./rc-mycatkubectl create -f ./svc-mycat 参照资料：利用Kubernetes搭建mysql主从复制集群基于Mycat的MySQL主从读写分离及自动切换的docker实现基于Docker容器的MyCat高可用方案centos7配置mysql主从复制Mycat官网mycat实现mysql读写分离，热切换，集群使用mycat实现mysql读写分离以及主备自动切换模式mycat读写分离配置]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hyperledger Hackathon]]></title>
      <url>%2F2017%2F03%2F15%2Fhyperledger%2F</url>
      <content type="text"><![CDATA[周末参加了万达和IBM组织的Hyperledger Hackathon，确实是一场区块链的盛宴，三十四个参赛队，过程紧张而热烈。我们也在24小时内完成产品设计，前后端的开发，联调和部署，PPT的制作，虽未得奖，但收获颇丰。相关代码和文档地址：HyperledgerHackathon。 1. 区块链概述先说说什么是区块链，区块链属于一种去中心化的记录技术。参与到系统上的节点，可能不属于同一组织、彼此无需信任；区块链数据由所有节点共同维护，每个参与维护节点都能复制获得一份完整记录的拷贝。具有以下特点： 维护一条不断增长的链，只可能添加记录，而发生过的记录都不可篡改； 去中心化，或者说多中心化，无需集中的控制而能达成共识，实现上尽量分布式； 通过密码学的机制来确保交易无法抵赖和破坏，并尽量保护用户信息和记录的隐私性。 更多区块链相关可参考：区块链技术指南。 2. 积分链我们的场景是积分链，基于区块链的垂直电商积分共享平台。 2.1 现状 消费者：消费者拥有积分种类繁多，但积分价值低，很多积分成为”鸡肋“ 商户：商户发行的积分流动性差，对消费者吸引力有限，小商户甚至无力发行积分。 现有积分平台：现有积分平台结算复杂，安全性存在隐患，导流效果不明显。 2.2 积分链 去中心化：保持各垂直电商独立性 积分共享：消费者可使用任意商户的任意积分 快速结算：结算机制高效可靠 交易透明：交易记录清晰易查 安全信任：数据安全，防止篡改，不可撤销 2.3 业务模式2.3.1 商户加盟 约定积分价值、积分发行量，缴纳保证金。 创建独立节点。 生成智能合约，包含积分价值、积分发行量、发行方等。 2.3.2 积分产生 用户在商户产生积分时，调用平台接口，平台增加该用户在该商户积分。 商户积分总量达到发行量时，通知商户补充保证金，发行新积分。 2.3.3 积分使用 用户可选择任意商户积分使用。 使用后平台产生交易记录，包含用户/使用方/发行方/数量/订单号等。 2.3.4 结算平台根据交易记录与积分价值约定，以周期或实时方式与各商户进行结算。 2.3.5 交易查询商户可查询所有交易记录。 2.4 Chaincode现场使用的是IBM bluemix，我们项目chaincode如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352package main//WARNING - this chaincode's ID is hard-coded in chaincode_example04 to illustrate one way of//calling chaincode from a chaincode. If this example is modified, chaincode_example04.go has//to be modified as well with the new ID of chaincode_example02.//chaincode_example05 show's how chaincode ID can be passed in as a parameter instead of//hard-coding.import ( "errors" "fmt" "strconv" "encoding/json" "github.com/hyperledger/fabric/core/chaincode/shim")// SimpleChaincode example simple Chaincode implementationtype SimpleChaincode struct &#123;&#125;func (t *SimpleChaincode) Init(stub shim.ChaincodeStubInterface, function string, args []string) ([]byte, error) &#123; var username, shopA, shopB string // Entities var err error if len(args) != 3 &#123; return nil, errors.New("Incorrect number of arguments. Expecting 3: username, shopA, shopB") &#125; // Initialize the chaincode username = args[0] shopA = args[1] shopB = args[2] fmt.Printf("username = %s, shopA= %s, shopB = %s\n", username, shopA, shopB) var user_A, user_B, user_A_B, user_B_A string user_A = username + "_" + shopA user_B = username + "_" + shopB user_A_B = username + "_" + shopA + "_" + shopB user_B_A = username + "_" + shopB + "_" + shopA // Write the state to the ledger err = stub.PutState("user", []byte(username)) if err != nil &#123; return nil, err &#125; shops := []string&#123;shopA, shopB&#125; shopsBytes, _ := json.Marshal(shops) err = stub.PutState(username, shopsBytes) if err != nil &#123; return nil, err &#125; err = stub.PutState(user_A, []byte(strconv.Itoa(0))) if err != nil &#123; return nil, err &#125; err = stub.PutState(user_B, []byte(strconv.Itoa(0))) if err != nil &#123; return nil, err &#125; err = stub.PutState(user_A_B, []byte(strconv.Itoa(0))) if err != nil &#123; return nil, err &#125; err = stub.PutState(user_B_A, []byte(strconv.Itoa(0))) if err != nil &#123; return nil, err &#125; return nil, nil&#125;// Transaction// 1. add user, shop, points: add, username, shopName, xx points// 2. user1 spent shopA's points in shopB by xx points: consume, username, shopA, shopB, xx pointsfunc (t *SimpleChaincode) Invoke(stub shim.ChaincodeStubInterface, function string, args []string) ([]byte, error) &#123; fmt.Println("Invoke running. Function: " + function) if function == "add" &#123; return t.add(stub, args) &#125; else if function == "consume" || function == "spend" &#123; return t.spend(stub, args) &#125; return nil, errors.New("Received unknown function invocation: " + function)&#125;func (t *SimpleChaincode) add(stub shim.ChaincodeStubInterface, args []string) ([]byte, error) &#123; var username, shopname, user_shop string var points, cur_points int // accumulated points var err error if len(args) != 3 &#123; return nil, errors.New("Incorrect number of arguments. Expecting 3: username, shopname, points") &#125; username = args[0] shopname = args[1] points, _ = strconv.Atoi(args[2]) fmt.Println("add: got param: " + username + "," + shopname + "," + args[2]) user_shop = username + "_" + shopname pointsBytes, err := stub.GetState(user_shop) if err != nil &#123; return nil, errors.New("Failed to get state: " + user_shop) &#125; if pointsBytes == nil &#123; return nil, errors.New("Entity not found: " + user_shop) &#125; cur_points, _ = strconv.Atoi(string(pointsBytes)) cur_points = cur_points + points fmt.Printf("After add points:%d, cur_points:%d\n", points, cur_points) // Write the state back to the ledger err = stub.PutState(user_shop, []byte(strconv.Itoa(cur_points))) if err != nil &#123; return nil, err &#125; return nil, nil&#125;func (t *SimpleChaincode) spend(stub shim.ChaincodeStubInterface, args []string) ([]byte, error) &#123; var username, spend_shopname, spent_shopname, user_spent_shop, user_spend_spent_shop string var points, cur_points int // accumulated points var err error if len(args) != 4 &#123; return nil, errors.New("Incorrect number of arguments. Expecting 4: username, spend_shop, spent_shop, points") &#125; username = args[0] spend_shopname = args[1] spent_shopname = args[2] points, err = strconv.Atoi(args[3]) fmt.Println("spend: got param: " + username + "," + spend_shopname + "," + spent_shopname + ", " + args[3]) //user_spend_shop = username + "_" + spend_shopname user_spent_shop = username + "_" + spent_shopname user_spend_spent_shop = username + "_" + spend_shopname + "_" + spent_shopname // Subtract spent shop's points and record user_spend_spent_shop value. This is the points that is how many points shopA spend shopB var user_shop string user_shop = user_spent_shop pointsBytes, err := stub.GetState(user_shop) if err != nil &#123; return nil, errors.New("Failed to get state: " + user_shop) &#125; if pointsBytes == nil &#123; return nil, errors.New("Entity not found: " + user_shop) &#125; cur_points, _ = strconv.Atoi(string(pointsBytes)) cur_points = cur_points - points fmt.Printf("After spend points, user-shop's ledger:%s, points: %d, cur_points:%d\n", user_shop, points, cur_points) // Write the state back to the ledger err = stub.PutState(user_shop, []byte(strconv.Itoa(cur_points))) if err != nil &#123; return nil, err &#125; user_shop = user_spend_spent_shop pointsBytes, err = stub.GetState(user_shop) if err != nil &#123; return nil, errors.New("Failed to get state: " + user_shop) &#125; if pointsBytes == nil &#123; return nil, errors.New("Entity not found: " + user_shop) &#125; cur_points, _ = strconv.Atoi(string(pointsBytes)) cur_points = cur_points + points fmt.Printf("After spend points, user-shops' ledger:%s, points:%d, cur_points %s\n", user_shop, points, cur_points) // Write the state back to the ledger err = stub.PutState(user_shop, []byte(strconv.Itoa(cur_points))) if err != nil &#123; return nil, err &#125; return nil, nil&#125;// Query callback representing the query of a chaincodefunc (t *SimpleChaincode) Query(stub shim.ChaincodeStubInterface, function string, args []string) ([]byte, error) &#123; fmt.Println("Query running. Function: " + function) if function == "query_user" || function == "query" &#123; return t.query_user(stub, args) &#125; else if function == "query_shop" &#123; return t.query_shop(stub, args) &#125; return []byte("No such function"), nil&#125;func (t *SimpleChaincode) query_user(stub shim.ChaincodeStubInterface, args []string) ([]byte, error) &#123; var username, shopA, shopB, user_A, user_B string var shops []string var shopA_points, shopB_points int // accumulated points var err error if len(args) != 1 &#123; return nil, errors.New("Incorrect number of arguments. Expecting 1: username") &#125; username = args[0] fmt.Println("query_user: got param: " + username ) shopsBytes, err := stub.GetState(username) err = json.Unmarshal(shopsBytes, &amp;shops) if err != nil &#123; fmt.Println("Error unmarshalling user's shops: " + username + "\n---&gt;: " + err.Error()) return nil, errors.New("Error unmarshalling user's shops " + username) &#125; shopA = shops[0] shopB = shops[1] user_A = username + "_" + shopA user_B = username + "_" + shopB user_shop := user_A pointsBytes, err := stub.GetState(user_shop) if err != nil &#123; return nil, errors.New("Failed to get state: " + user_shop) &#125; if pointsBytes == nil &#123; return nil, errors.New("Entity not found: " + user_shop) &#125; shopA_points, _ = strconv.Atoi(string(pointsBytes)) user_shop = user_B pointsBytes, err = stub.GetState(user_shop) if err != nil &#123; return nil, errors.New("Failed to get state: " + user_shop) &#125; if pointsBytes == nil &#123; return nil, errors.New("Entity not found: " + user_shop) &#125; shopB_points, _ = strconv.Atoi(string(pointsBytes)) resp:= map[string]int&#123; shopA: shopA_points, shopB: shopB_points, &#125; jsonResp, err := json.Marshal(resp) if err != nil &#123; return nil, errors.New("resp marshal fail" ) &#125; fmt.Printf("Query Response:%s\n", jsonResp) return []byte(jsonResp), nil&#125;// // Settle shops' points//func (t *SimpleChaincode) query_shop(stub shim.ChaincodeStubInterface, args []string) ([]byte, error) &#123; var username, shopA, shopB, user_A, user_B string var shops []string var shopA_points, shopB_points int // accumulated points var err error if len(args) != 1 &#123; return nil, errors.New("Incorrect number of arguments 2. Expecting: shopA, shopB") &#125; username = "phyllis" shopA = args[0] shopB = args[1] fmt.Println("query_shop: got param: " + shopA + "," + shopB) //var user_A_B, user_B_A string user_A = username + "_" + shopA user_B = username + "_" + shopB //user_A_B = username + "_" + shopA + "_" + shopB //user_B_A = username + "_" + shopB + "_" + shopA shopsBytes, err := stub.GetState(username) err = json.Unmarshal(shopsBytes, &amp;shops) if err != nil &#123; fmt.Println("Error unmarshalling user's shops: " + username + "\n---&gt;: " + err.Error()) return nil, errors.New("Error unmarshalling user's shops " + username) &#125; shopA = shops[0] shopB = shops[1] user_A = username + "_" + shopA user_B = username + "_" + shopB user_shop := user_A pointsBytes, err := stub.GetState(user_shop) if err != nil &#123; return nil, errors.New("Failed to get state: " + user_shop) &#125; if pointsBytes == nil &#123; return nil, errors.New("Entity not found: " + user_shop) &#125; shopA_points, _ = strconv.Atoi(string(pointsBytes)) user_shop = user_B pointsBytes, err = stub.GetState(user_shop) if err != nil &#123; return nil, errors.New("Failed to get state: " + user_shop) &#125; if pointsBytes == nil &#123; return nil, errors.New("Entity not found: " + user_shop) &#125; shopB_points, _ = strconv.Atoi(string(pointsBytes)) resp:= map[string]int&#123; shopA: shopA_points, shopB: shopB_points, &#125; jsonResp, err := json.Marshal(resp) if err != nil &#123; return nil, errors.New("resp marshal fail" ) &#125; fmt.Printf("Query Response:%s\n", jsonResp) return []byte(jsonResp), nil&#125;func main() &#123; err := shim.Start(new(SimpleChaincode)) if err != nil &#123; fmt.Printf("Error starting Simple chaincode: %s", err) &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ansible部署Kubernetes集群]]></title>
      <url>%2F2017%2F02%2F03%2Fk8sansible%2F</url>
      <content type="text"><![CDATA[Kubernetes官网提供了多种场景的部署方案，例如kubeadm，参照：Creating a Cluster。但还是觉得ansible的部署最方便，k8s的contrib中提供了ansible部署脚本，使用方法参照：Kubernetes Ansible。 1. 服务器构成 No Type IP Hostname OS 1 master 192.168.34.52 cts1 Centos7 2 etcd 192.168.34.52 cts1 Centos7 3 minion 192.168.34.180 cts2 Centos7 2. 安装ansible在master上安装ansible：1[root@cts1 ~]# yum -y install ansible安装后确认：1234[root@cts1 ~]# ansible --versionansible 2.2.0.0 config file = /etc/ansible/ansible.cfg configured module search path = Default w/o overrides 3. 设置ssh该部署方案需要使用ssh免密登录，在所有机器上生成ssh key:123[root@cts1 ~]# ssh-keygen[root@cts2 ~]# ssh-keygen可以通过hostname [newname]修改hostname，设定两台机器的/etc/hosts :1234567[root@cts1 ~]# grep cts /etc/hosts192.168.34.52 cts1192.168.34.180 cts2[root@cts2 ~]# grep cts /etc/hosts192.168.34.52 cts1192.168.34.180 cts2在2台机器上都作如下设定，保证ssh可免密登录:12345[root@cts1 ~]# ssh-copy-id -i cts1[root@cts1 ~]# ssh-copy-id -i cts2[root@cts2 ~]# ssh-copy-id -i cts1[root@cts2 ~]# ssh-copy-id -i cts2在ansible所安装的机器上，追加机器信息到/etc/ansible/hosts中:123[root@cts1 ~]# grep cts /etc/ansible/hostscts1cts2确认ansible正常动作:12345678910[root@cts1 ~]# ansible cts1 -m pingcts1 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;[root@cts1 ~]# ansible cts2 -m pingcts2 | SUCCESS =&gt; &#123; &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 4. 下载Contrib1[root@cts1 ~]# git clone https://github.com/kubernetes/contrib.git 5. 创建inventory文件按照contrib/ansible说明，在inventory目录下创建inventory文件：123456789101112[root@cts1 inventory]# pwd/data/contrib/ansible/inventory[root@cts1 inventory]# vim inventory[masters]cts1[etcd:children]masters[nodes]cts2开始误把etcd:children的值也配成cts1了，setup时报出Syntax Error while loading YAML的错误。 6. 安装所需package1[root@cts1 inventory]# yum -y install python-netaddr 7. 执行安装文件执行cd scripts/ &amp;&amp; ./deploy-cluster.sh时总是报出如下错误：12TASK [kubernetes : Read back the CA certificate] fatal: [cts2 -&gt; cts1]: FAILED! =&gt; &#123;&quot;changed&quot;: false, &quot;failed&quot;: true, &quot;msg&quot;: &quot;file not found: /etc/kubernetes/certs/ca.crt&quot;&#125;于是按照Targeted runs逐个执行：Etcd:1234[root@cts1 scripts]# ./deploy-cluster.sh --tags=etcdcts1 : ok=28 changed=2 unreachable=0 failed=0cts2 : ok=6 changed=0 unreachable=0 failed=0确认etcd:12345678[root@cts1 scripts]# etcd --versionetcd Version: 3.0.15Git SHA: fc00305Go Version: go1.6.3Go OS/Arch: linux/amd64[root@cts1 scripts]# etcdctl --versionetcdctl version: 3.0.15API version: 2Kubernetes master:1234[root@cts1 scripts]# ./deploy-cluster.sh --tags=masterscts1 : ok=67 changed=4 unreachable=0 failed=0cts2 : ok=6 changed=0 unreachable=0 failed=0Kubernetes nodes:目前contrib版本中Install fluentd时因fluentd-es.yaml地址已不存在，会出错，解决办法参照：Support Fluentd migration to DaemonSet，将如下文件进行修改： ansible/roles/kubernetes-addons/files/common/kube-addon-update.sh ansible/roles/kubernetes-addons/tasks/cluster-logging.yml ansible/roles/kubernetes-addons/templates/cluster-logging/fluentd-es-ds.yaml.j2 ansible/roles/node/tasks/fluentd-install.yml ansible/roles/node/tasks/main.yml 然后执行安装命令：1234[root@cts1 scripts]# ./deploy-cluster.sh --tags=nodescts1 : ok=8 changed=0 unreachable=0 failed=0cts2 : ok=57 changed=3 unreachable=0 failed=0Addons:1234[root@cts1 scripts]# ./deploy-cluster.sh --tags=addonscts1 : ok=58 changed=1 unreachable=0 failed=0cts2 : ok=6 changed=0 unreachable=0 failed=0 8. 确认Kubernetes状况12345678[root@cts1 scripts]# kubectl get nodesNAME STATUS AGEcts2 Ready 2h[root@cts1 scripts]# kubectl get servicesNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.254.0.1 &lt;none&gt; 443/TCP 2h[root@cts1 scripts]# kubectl --versionKubernetes v1.4.0 参照资料：Kubernetes 1.3 从入门到进阶 安装篇（2）Kubernetes AnsibleSupport Fluentd migration to DaemonSet #2174]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes + Docker搭建Sonarqube]]></title>
      <url>%2F2017%2F01%2F03%2Fsonarqube%2F</url>
      <content type="text"><![CDATA[1. Sonar概述Sonar是一个用于代码质量管理的开放平台。通过插件机制，Sonar可以集成不同的测试工具，代码分析工具，以及持续集成工具。与持续集成工具（例如Hudson/Jenkins等）不同，Sonar并不是简单地把不同的代码检查工具结果（例如FindBugs，PMD等）直接显示在Web页面上，而是通过不同的插件对这些结果进行再加工处理，通过量化的方式度量代码质量的变化，从而可以方便地对不同规模和种类的工程进行代码质量管理。在对其他工具的支持方面，Sonar不仅提供了对IDE的支持，可以在Eclipse和IntelliJ IDEA这些工具里联机查看结果；同时Sonar 还对大量的持续集成工具提供了接口支持，可以很方便地在持续集成中使用Sonar。此外，Sonar的插件对Java/PHP/C/C++/Python等编程语言提供支持，对国际化以及报告文档化也有良好的支持。官网：Sonarqube 2. Snoar结构与集成结构图： 与其他ALM工具集成： 3. Sonar的Kubernetes部署3.1 事前准备集群构成：建议先将sonarqube镜像pull下来。不准备的话kubernetes创建RC的时候也会自动pull。1docker pull sonarqube 3.2 创建namespace定义namespace.yaml：namespace.yaml1234apiVersion: v1kind: Namespacemetadata: name: sonar创建sonar命名空间：1kubectl create -f namespace.yaml 3.3 启动mysql首先要启动db，这里使用mysql。注意：sonarqube6.2镜像要求mysql版本是5.6以上，所以mariadb不可以。另外max_allowed_packet需要设置的大一些，mysql镜像默认是4M。 rc-mysql56.yaml1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: ReplicationControllermetadata: name: rc-mysql56 namespace: sonar labels: name: mysql context: ctpaas_staging_sitespec: replicas: 1 template: metadata: labels: name: mysql spec: containers: - name: mysql image: docker.io/mysql args: ["--character-set-server=utf8mb4", "--collation-server=utf8mb4_unicode_ci", "--max_allowed_packet=32M"] env: - name: TZ value: 'Asia/Shanghai' - name: LANG value: 'C.UTF-8' - name: MYSQL_ROOT_PASSWORD value: 'password' - name: MYSQL_DATABASE value: 'sonar' ports: - containerPort: 3306 volumeMounts: - mountPath: "/var/lib/mysql" name: mysql-data-dir volumes: - name: mysql-data-dir nfs: server: 192.168.205.77 path: "/nfs_share/mysql56/mysql_data" svc-mysql56.yaml1234567891011121314151617181920apiVersion: v1kind: Servicemetadata: name: svc-mysql56 namespace: sonarspec: ports: - port: 3306 # the port that this service should serve on # the container on each pod to connect to, can be a name # (e.g. 'www') or a number (e.g. 80) targetPort: 3306 protocol: TCP nodePort: 0 # just like the selector in the replication controller, # but this time it identifies the set of pods to load balance # traffic to. selector: name: mysql type: NodePort 创建mysql rc和service：12kubectl create -f rc-mysql56.yamlkubectl create -f svc-mysql56.yaml 3.3 创建Sonar RC/Service定义rc-sonar.yaml，db使用mysql的cluster ip：rc-sonar.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354apiVersion: v1kind: ReplicationControllermetadata: name: rc-sonar namespace: sonar labels: name: sonarspec: replicas: 1 selector: name: sonar template: metadata: labels: name: sonar spec: containers: - name: sonar image: docker.io/sonarqube env: - name: SONARQUBE_JDBC_USERNAME value: user - name: SONARQUBE_JDBC_PASSWORD value: password - name: SONARQUBE_JDBC_URL value: 'jdbc:mysql://10.254.130.11:3306/sonar?useUnicode=true&amp;amp;characterEncoding=utf8' ports: - containerPort: 9000 volumeMounts: - mountPath: "/opt/sonarqube/conf" name: sonar-conf - mountPath: "/opt/sonarqube/data" name: sonar-data - mountPath: "/opt/sonarqube/extensions" name: sonar-extensions - mountPath: "/opt/sonarqube/logs" name: sonar-logs volumes: - name: sonar-conf nfs: server: 192.168.205.77 path: "/nfs_share/sonar/conf" - name: sonar-data nfs: server: 192.168.205.77 path: "/nfs_share/sonar/data" - name: sonar-extensions nfs: server: 192.168.205.77 path: "/nfs_share/sonar/extensions" - name: sonar-logs nfs: server: 192.168.205.77 path: "/nfs_share/sonar/logs"创建volumes，将数据、配置、插件、日志都放在volumes上，这样容器重新创建也不会受影响。 定义svc-sonar.yaml:svc-sonar.yaml12345678910111213apiVersion: v1kind: Servicemetadata: name: svc-sonar namespace: sonarspec: type: NodePort ports: - port: 9000 protocol: TCP nodePort: 0 selector: name: sonar 创建sonar rc和service：12kubectl create -f rc-sonar.yamlkubectl create -f svc-sonar.yaml 确认pod/rc/svc: 获取sonar service的NodePort: 确认是否可以正常打开： 4. 使用Sonar进行代码分析4.1 安装Sonarqube Scanner参照Analyzing with SonarQube Scanner: 下载Scanner并解压 修改install_directory/conf/sonar-scanner.properties中的sonar.host.url为搭建的sonar服务器地址，端口为NodePort 将install_directory/bin加入path中 运行sonar-scanner -h 4.2 创建sonar-project.properties在项目根目录下添加sonar-project.properties配置文件，模板如下：sonar-project.properties12345678910111213141516171819202122232425#required metadata#projectKey项目的唯一标识，不能重复。sonar.projectKey=pj_key#projectName值不能是中文，否则web页面部分是乱码sonar.projectName=pj_namesonar.projectVersion=1.0sonar.sourceEncoding=UTF-8sonar.modules=java-module,javascript-module,html-module# Java modulejava-module.sonar.projectName=pj_name_javajava-module.sonar.language=java# .表示projectBaseDir指定的目录java-module.sonar.sources=JavaSourcejava-module.sonar.projectBaseDir=.#sonar.binaries=classes# JavaScript modulejavascript-module.sonar.projectName=pj_name_jsjavascript-module.sonar.language=jsjavascript-module.sonar.sources=WebContentjavascript-module.sonar.projectBaseDir=.# Html modulehtml-module.sonar.projectName=pj_name_htmlhtml-module.sonar.language=webhtml-module.sonar.sources=WebContenthtml-module.sonar.projectBaseDir=. 4.3 分析并确认结果在代码根目录运行sonar-scanner，结束后到sonar上确认结果： 5. Jenkins集成Sonarqube参照Analyzing with SonarQube Scanner for Jenkins，在Jenkins上安装Sonarqube插件后进行配置。具体后面再开一篇说明。 6. Sonar常用插件目前安装的插件如下： 参照资料：Kubernetes单Pod启动sonarqubeFrom Pet to Cattle – Running Sonar on Kubernetessonar-examplesSonarQube Documentation]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java编译和运行]]></title>
      <url>%2F2016%2F09%2F07%2Fjavabuildandrun%2F</url>
      <content type="text"><![CDATA[整理一下Java编译和运行的大致过程。先上一张Java平台逻辑结构图： 1. 概述如图， 从上至下分别是编译型语言，解释型语言，Java的执行模型。Java程序从源文件创建到程序运行要经过两大步骤：1、源文件由编译器编译成字节码（ByteCode） 2、字节码由java虚拟机解释运行。关于Javay属于编译型语言还是解释性语言，可以参照Java 是编译型语言还是解释型语言。 2. 编译Java代码编译是由Java源码编译器来完成，流程图如下所示： 2.1 编译机制Java 源码编译由以下三个过程组成： 分析和输入到符号表 注解处理 语义分析和生成class文件 最后生成的class文件由以下部分组成： 结构信息。包括class文件格式版本号及各部分的数量与大小的信息 元数据。对应于Java源码中声明与常量的信息。包含类/继承的超类/实现的接口的声明信息、域与方法声明信息和常量池 方法信息。对应Java源码中语句和表达式对应的信息。包含字节码、异常处理器表、求值栈与局部变量区大小、求值栈的类型记录、调试符号信息 2.2 字节码Class文件是8位字节流，按字节对齐。之所以称为字节码，是因为每条指令都只占据一个字节，所有的操作码和操作数都是按字节对齐的。Java虚拟机规范中规定，Class文件格式采用一种类似C语言结构体的伪结构来存储，它只有两种数据类型 无符号数（基本数据类型）主要用于描述数字、索引引用、数量值、或UTF-8编码构成的字符串；u1 – 1个字节u2 – 2个字节u4 – 4个字节u8 – 8个字节 表（符合数据类型）用于描述有层次关系的符合结构的数据；习惯性以“_info”结尾 更详细介绍可以参照;Java字节码小结Java代码到字节码Java字节码.class文件案例分析 3. 运行Java字节码的执行是由JVM执行引擎来完成，流程图如下所示： 3.1 什么是JVMJVM是Java Virtual Machine（Java虚拟机）的缩写，JVM是一种用于计算设备的规范，它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。Java虚拟机包括一套字节码指令集、一组寄存器、一个栈、一个垃圾回收堆和一个存储方法域。JVM屏蔽了与具体操作系统平台相关的信息，使Java程序只需生成在Java虚拟机上运行的目标代码（字节码）,就可以在多种平台上不加修改地运行。JVM在执行字节码时，实际上最终还是把字节码解释成具体平台上的机器指令执行。JVM规范参照：Java Virtual Machine Specification中文版：Java虚拟机规范（JavaSE7） 3.2 JRE/JDK/JVM的关系 JRE(JavaRuntimeEnvironment，Java运行环境)，也就是Java平台。所有的Java程序都要在JRE下才能运行。普通用户只需要运行已开发好的java程序，安装JRE即可。 JDK(Java Development Kit)是程序开发者用来来编译、调试java程序用的开发工具包。JDK的工具也是Java程序，也需要JRE才能运行。为了保持JDK的独立性和完整性，在JDK的安装过程中，JRE也是 安装的一部分。所以，在JDK的安装目录下有一个名为jre的目录，用于存放JRE文件。 JVM(JavaVirtualMachine，Java虚拟机)是JRE的一部分。它是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。JVM有自己完善的硬件架构，如处理器、堆栈、寄存器等，还具有相应的指令系统。Java语言最重要的特点就是跨平台运行。使用JVM就是为了支持与操作系统无关，实现跨平台。 3.3 JVM架构3.3.1 JVM体系结构JVM = 类加载器 classloader + 执行引擎 execution engine + 运行时数据区域 runtime data area 类装载器（ClassLoader）（用来装载.class文件） 执行引擎（执行字节码，或者执行本地方法） 运行时数据区（方法区、堆、java栈、PC寄存器、本地方法栈）JVM体系结构： 3.3.2 classloaderclassloader作用是装载.class文件。classloader 有两种装载class的方式（时机）： 隐式：运行过程中，碰到new方式生成对象时，隐式调用classLoader到JVM 显式：通过class.forname()动态加载类的层次关系和加载顺序可以由下图来描述： 1）Bootstrap ClassLoader负责加载 $JAVA_HOME中jre/lib/rt.jar里所有的class，由C++实现，不是ClassLoader子类2）Extension ClassLoader负责加载java平台中扩展功能的一些jar包，包括$JAVA_HOME中jre/lib/*.jar或-Djava.ext.dirs指定目录下的jar包3）App ClassLoader负责记载classpath中指定的jar包及目录中class4）Custom ClassLoader属于应用程序根据自身需要自定义的ClassLoader，如tomcat、jboss都会根据j2ee规范自行实现ClassLoader加载过程中会先检查类是否被已加载，检查顺序是自底向上，从Custom ClassLoader到BootStrap ClassLoader逐层检查，只要某个classloader已加载就视为已加载此类，保证此类只所有ClassLoader加载一次。而加载的顺序是自顶向下，也就是由上层来逐层尝试加载此类。 3.3.3 执行引擎执行字节码，或者执行本地方法。类被加载到虚拟机内存中开始，到卸载出内存为主，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）、初始化（Initialization）、使用（Using）和卸载（Unloading）7个阶段。其中验证、准备、解析3个阶段称为连接（Linking）。 3.3.4 运行时数据区JVM运行时数据区： PC程序计数器：一块较小的内存空间，可以看做是当前线程所执行的字节码的行号指示器, NAMELY存储每个线程下一步将执行的JVM指令，如该方法为native的，则PC寄存器中不存储任何信息。Java 的多线程机制离不开程序计数器，每个线程都有一个自己的PC，以便完成不同线程上下文环境的切换。 java虚拟机栈：与PC 一样，java虚拟机栈也是线程私有的。每一个JVM线程都有自己的java虚拟机栈，这个栈与线程同时创建，它的生命周期与线程相同。虚拟机栈描述的是Java 方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部基本类型的变量（java中定义的八种基本类型：boolean、char、byte、short、int、long、float、double）、部分的返回结果以及Stack Frame，非基本类型的对象在JVM栈上仅存放一个指向堆上的地址。。每一个方法被调用直至执行完成的过程就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 堆（Heap）:它是JVM用来存储对象实例以及数组值的区域，可以认为Java中所有通过new创建的对象的内存都在此分配，Heap中的对象的内存需要等待GC进行回收。1）堆是JVM中所有线程共享的，因此在其上进行对象内存的分配均需要进行加锁，这也导致了new对象的开销是比较大的2）Sun Hotspot JVM为了提升对象内存分配的效率，对于所创建的线程都会分配一块独立的空间TLAB（Thread Local Allocation Buffer），其大小由JVM根据运行的情况计算而得，在TLAB上分配对象时不需要加锁，因此JVM在给线程的对象分配内存时会尽量的在TLAB上分配，在这种情况下JVM中分配对象内存的性能和C基本是一样高效的，但如果对象过大的话则仍然是直接使用堆空间分配3）TLAB仅作用于新生代的Eden Space，因此在编写Java程序时，通常多个小的对象比大的对象分配起来更加高效。4）所有新创建的Object 都将会存储在新生代Yong Generation中。如果Young Generation的数据在一次或多次GC后存活下来，那么将被转移到OldGeneration。新的Object总是创建在Eden Space。 方法区域（Method Area）1）在Sun JDK中这块区域对应的为PermanetGeneration，又称为持久代。2）方法区域存放了所加载的类的信息（名称、修饰符等）、类中的静态变量、类中定义为final类型的常量、类中的Field信息、类中的方法信息，当开发人员在程序中通过Class对象中的getName、isInterface等方法来获取信息时，这些数据都来源于方法区域，同时方法区域也是全局共享的，在一定的条件下它也会被GC，当方法区域需要使用的内存超过其允许的大小时，会抛出OutOfMemory的错误信息。 运行时常量池（Runtime Constant Pool）存放的为类中的固定的常量信息、方法和Field的引用信息等，其空间从方法区域中分配。 本地方法堆栈（Native Method Stacks）JVM采用本地方法堆栈来支持native方法的执行，此区域用于存储每个native方法调用的状态。 3.4 JVM垃圾回收这里对垃圾回收作简单介绍，后面再详细展开。GC (Garbage Collection)的基本原理：将内存中不再被使用的对象进行回收，GC中用于回收的方法称为收集器，由于GC需要消耗一些资源和时间，Java在对对象的生命周期特征进行分析后，按照新生代、旧生代的方式来对对象进行收集，以尽可能的缩短GC对应用造成的暂停。1）对新生代的对象的收集称为minor GC；2）对旧生代的对象的收集称为Full GC；3）程序中主动调用System.gc()强制执行的GC为Full GC。 不同的对象引用类型， GC会采用不同的方法进行回收，JVM对象的引用分为了四种类型：1）强引用：默认情况下，对象采用的均为强引用（这个对象的实例没有其他对象引用，GC时才会被回收）2）软引用：软引用是Java中提供的一种比较适合于缓存场景的应用（只有在内存不够用的情况下才会被GC）3）弱引用：在GC时一定会被GC回收4）虚引用：由于虚引用只是用来得知对象是否被GC 4. 实例1234567891011121314151617//MainApp.java public class MainApp &#123; public static void main(String[] args) &#123; Animal animal = new Animal("Puppy"); animal.printName(); &#125; &#125; //Animal.java public class Animal &#123; public String name; public Animal(String name) &#123; this.name = name; &#125; public void printName() &#123; System.out.println("Animal ["+name+"]"); &#125; &#125; 第一步(编译): 创建完源文件之后，程序会先被编译为.class文件。Java编译一个类时，如果这个类所依赖的类还没有被编译，编译器就会先编译这个被依赖的类，然后引用，否则直接引用，这个有点象make。如果java编译器在指定目录下找不到该类所其依赖的类的.class文件或者.java源文件的话，编译器话报“cant find symbol”的错误。 编译后的字节码文件格式主要分为两部分：常量池和方法字节码。常量池记录的是代码出现过的所有token(类名，成员变量名等等)以及符号引用（方法引用，成员变量引用等等）；方法字节码放的是类中各个方法的字节码。下面是MainApp.class通过反汇编的结果，我们可以清楚看到.class文件的结构：MainApp类常量池:MainApp类方法字节码: 第二步（运行）：java类运行的过程大概可分为两个过程：1、类的加载 2、类的执行。需要说明的是：JVM主要在程序第一次主动使用类的时候，才会去加载该类。也就是说，JVM并不是在一开始就把一个程序就所有的类都加载到内存中，而是到不得不用的时候才把它加载进来，而且只加载一次。下面是程序运行的详细步骤： 在编译好java程序得到MainApp.class文件后，在命令行上敲java AppMain。系统就会启动一个jvm进程，jvm进程从classpath路径中找到一个名为AppMain.class的二进制文件，将MainApp的类信息加载到运行时数据区的方法区内，这个过程叫做MainApp类的加载。 然后JVM找到AppMain的主函数入口，开始执行main函数。 main函数的第一条命令是Animal animal = new Animal(“Puppy”);就是让JVM创建一个Animal对象，但是这时候方法区中没有Animal类的信息，所以JVM马上加载Animal类，把Animal类的类型信息放到方法区中。 加载完Animal类之后，Java虚拟机做的第一件事情就是在堆区中为一个新的Animal实例分配内存, 然后调用构造函数初始化Animal实例，这个Animal实例持有着指向方法区的Animal类的类型信息（其中包含有方法表，java动态绑定的底层实现）的引用。 当使用animal.printName()的时候，JVM根据animal引用找到Animal对象，然后根据Animal对象持有的引用定位到方法区中Animal类的类型信息的方法表，获得printName()函数的字节码的地址。 开始运行printName()函数。 参照资料：JVM的工作原理，层次结构以及GC工作原理JVM介绍Java代码编译和执行过程JVM学习笔记（二）——Java代码编译和执行的整个过程JVM Internals]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[windows下搭建python开发环境]]></title>
      <url>%2F2016%2F02%2F23%2Fwinpyconfig%2F</url>
      <content type="text"><![CDATA[安装python到官网下载安装包后安装，以2.7为例，安装完成后在环境变量Path中添加C:\Python27;C:\Python27\Scripts。在cmd中输入python -V,输出版本信息则安装成功。 创建virtualenv虚拟环境2.1 安装virtualenvpython 2.7中包含了pip，可以在cmd下输入pip -V验证。执行：1pip install virtualenv如需代理，windows下可执行：1pip --proxy proxy:port install virtualenvlinux下：12export https_proxy=proxy:portsudo -E pip insatll virtualenv安装完后输入virtualenv –version验证。 可以安装virtualenvwrapper进行更方便的操作，windows下是virtualenvwrapper-win。Virtaulenvwrapper是virtualenv的扩展包，用于更方便管理虚拟环境，它可以做： 将所有虚拟环境整合在一个目录下 管理（新增，删除，复制）虚拟环境 切换虚拟环境 123456789C:\Work\WorkProject\Python&gt;pip --proxy proxy:port install virtualenvwrapper-winYou are using pip version 7.0.1, however version 7.1.2 is available.You should consider upgrading via the 'pip install --upgrade pip' command.Collecting virtualenvwrapper-win Downloading virtualenvwrapper-win-1.2.1.zipRequirement already satisfied (use --upgrade to upgrade): virtualenv in c:\python27\lib\site-packages (from virtualenvwrapper-win)Installing collected packages: virtualenvwrapper-win Running setup.py install for virtualenvwrapper-winSuccessfully installed virtualenvwrapper-win-1.2.1 2.2 创建虚拟环境创建一个目录用来保存虚拟环境，进入该目录并在cmd下执行：123456789C:\Work\WorkProject\Python&gt;mkvirtualenv dayaenvNew python executable in dayaenv\Scripts\python.exeInstalling setuptools, pip, wheel...done.(dayaenv) C:\Work\WorkProject\Python&gt;lsvirtualenvdir /b /ad "C:\Users\jacob\Envs"======================================================dayaenv退出虚拟环境：123(dayaenv) C:\Work\WorkProject\Python&gt;deactivateC:\Work\WorkProject\Python&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[微信支付开发配置]]></title>
      <url>%2F2016%2F01%2F06%2Fwechatconfig%2F</url>
      <content type="text"><![CDATA[微信开发涉及到支付业务，配置相对繁琐一些，并且公众平台和商户平台要分别设置，如果第一次配可能会踩到很多坑，大体记录下主要的配置。 公众平台开发基本配置在开发-基本配置中，启用服务器配置，设置所开发系统后台的服务器地址、Token、EncodingAESKey、消息加解密方式。 JS接口安全域名在公众号设置-功能设置中，将所开发系统的域名添加到JS接口安全域名中，格式为域名全称，如www.abc.com，否则调用微信接口失败。 网页授权获取用户基本信息在接口权限-网页授权获取用户基本信息中设置系统域名，否则授权不通过，会报redirect_uri参数错误。 微信支付在微信支付-开发配置中，设置支付授权目录，该目录内容为发起支付所在目录，如WxpayAPI. 商户平台公众平台配置结束后，进入商户平台进一步配置，目前商户平台只支持IE。 安装操作证书在操作证书中安装操作证书。 API证书&amp;密钥在API安全-API证书中下载证书：在API安全-API密钥中设置密钥： 微信参数调用微信接口需要的基本参数： AppID AppSecret API密钥 微信支付商户号 API证书 到此微信侧基本配置完成，剩下的就是祈祷了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo的next主题]]></title>
      <url>%2F2015%2F12%2F12%2Fnexttheme%2F</url>
      <content type="text"><![CDATA[每次心血澎湃的换用新环境新工具后，总会迷失在选主题的漩涡中，不能摆脱，无法自拔。尝试了几款主题，本想用炫酷的yelee，一番挣扎后，还是决定使用next主题，感谢IIssNan。 Next主题下载这里使用的是Next的双栏皮肤Pisces，因还未正式发布，所以在Branch Pisces中下载，解压后放到\Hexo\themes目录下，然后hexo的_config.yml中改为theme: next-pisces，重新生成即可。 Next配置配置基本都是参照主题的官方文档来的：http://theme-next.iissnan.com/支持的比较全面，写的也很详细。阅读次数需要注册LeanCloud，配置方法参照这里。SEO优化参照arao的方法。 一通配置后，基本就是现在的模样了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Groovy文件处理]]></title>
      <url>%2F2015%2F09%2F22%2FGroovy%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%2F</url>
      <content type="text"><![CDATA[Groovy与其他动态语言一样，具有强大的文件/文本处理能力。 1. 遍历某文件夹内文件并进行文本查找与替换：1.1 遍历文件夹内所有文件并查找与替换：123456789101112def static replaceDirFileText(String src, String dest, File dir)&#123; def fileText dir.eachFileRecurse( &#123;file -&gt; if(file.isFile())&#123; fileText = file.text; fileText = fileText.replaceAll(src, dest) file.write(fileText, “utf-8”); &#125; &#125; )&#125; 1.2 遍历指定类型文件并查找与替换：12345678910111213141516171819def static replaceDirFileText(String src, String dest, File dir)&#123; def fileText //Replace the text in special file types //def backupFile def exts = [".java", ".xml"] dir.eachFileRecurse( &#123;file -&gt; for (ext in exts)&#123; if (file.name.endsWith(ext)) &#123; fileText = file.text; //backupFile = new File(file.path + ".bak"); //backupFile.write(fileText); fileText = fileText.replaceAll(src, dest) file.write(fileText, "utf-8"); &#125; &#125; &#125; )&#125; 2. 文件/文件夹复制。JAVA实现比较繁琐，Groovy简洁明了：2.1 Java实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public static void copyFile(File sourceFile, File targetFile) throws IOException &#123; BufferedInputStream inBuff = null; BufferedOutputStream outBuff = null; try &#123; // 新建文件输入流并对它进行缓冲 inBuff = new BufferedInputStream(new FileInputStream(sourceFile)); // 新建文件输出流并对它进行缓冲 outBuff = new BufferedOutputStream(new FileOutputStream(targetFile)); // 缓冲数组 byte[] b = new byte[1024 * 5]; int len; while ((len = inBuff.read(b)) != -1) &#123; outBuff.write(b, 0, len); &#125; // 刷新此缓冲的输出流 outBuff.flush(); &#125;catch (Exception e)&#123; println e e.printStackTrace() &#125;finally &#123; // 关闭流 if (inBuff != null) inBuff.close(); if (outBuff != null) outBuff.close(); &#125; &#125; public static void copyDirectiory(String sourceDir, String targetDir) throws IOException &#123; try &#123; // 新建目标目录 (new File(targetDir)).mkdirs(); // 获取源文件夹当前下的文件或目录 File[] file = (new File(sourceDir)).listFiles(); for (int i = 0; i &lt; file.length; i++) &#123; if (file[i].isFile()) &#123; // 源文件 File sourceFile=file[i]; // 目标文件 File targetFile=new File(new File(targetDir).getAbsolutePath() + File.separator+file[i].getName()); copyFile(sourceFile,targetFile); &#125; if (file[i].isDirectory()) &#123; // 准备复制的源文件夹 String dir1=sourceDir + "/" + file[i].getName(); // 准备复制的目标文件夹 String dir2=targetDir + "/"+ file[i].getName(); copyDirectiory(dir1, dir2); &#125; &#125; &#125;catch (Exception e)&#123; println e e.printStackTrace() &#125; &#125; 2.2 Groovy AntBuilder实现：2.2.1 Copy directory to another directory:12345String sourceDir = SOURCE_DIR_PATHString destinationDir = DESTINATION_DIR_PATHnew AntBuilder().copy(todir: destinationDir) &#123; fileset(dir: sourceDir)&#125; 2.2.2 Copy directory with excluding some files :123456789101112131415String sourceDir = SOURCE_DIR_PATHString destinationDir = DESTINATION_DIR_PATHnew AntBuilder().copy(todir: destinationDir) &#123; fileset(dir : sourceDir) &#123; exclude(name:"*.java") &#125;&#125;new AntBuilder().copy(todir: "E:/2") &#123; fileset(dir : "E:/1") &#123; include(name:"**/*.java") exclude(name:"**/*Test.java") &#125;&#125; 2.2.3 Copy files from one directory to another :123String sourceFilePath = SOURCE_FILE_PATHString destinationFilePath = DESTINATION_FILE_PATH(new AntBuilder()).copy(file: sourceFilePath, tofile: destinationFilePath) 参考:http://www.codercorp.com/blog/groovy/groovy-file-magic-with-antbuilder.htmlhttp://www.grails.info/2012/09/21/copy-filesfolders-from-one-location-to-another-in-groovy/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Hexo在Github搭建博客]]></title>
      <url>%2F2015%2F09%2F01%2F%E4%BD%BF%E7%94%A8Hexo%E5%9C%A8Github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
      <content type="text"><![CDATA[倒腾了一天终于把blog搭起来了，第一篇就记录下搭建方法吧。 1. 购买域名域名是在Godaddy上买的，原因是国外且支持支付宝，比较方便。 注册个账号，注意个人信息中国家要选China，否则没有支付宝选项。 查到域名加入购物车，选完后会有一堆收费服务，不用管，直接Continue to Cart。 确认域名、购买时间等，然后一路点下去，支付方式选择支付宝，支付后购买成功。 这里说明一下，币种默认USD就可以，支付宝会自动根据汇率转换。另外比较坑的就是优惠码，网上随便一搜一堆，我挑了几个说是支持支付宝的，可在跳转支付界面时就提示错误了，换了几个都这样，不知道是使用的人数已经超额还是压根就不支持，anyway，删掉优惠码后就成功支付了。优惠码添加和删除在这里： 2. GitHub注册账号，创建username.github.ioGitHub提供了Pages服务，可以用来搭建静态博客。官方介绍很详细，可以参照：GitHub Pages 注册账号 选择”+New repository” Repository name一定要填username.github.io(其中username替换为你github注册的实际用户名) 参照The Automatic Page Generator自动生成Page。 这时候从浏览器访问username.github.io就可以看到自动生成的page了。 3. 安装git，nodejs，hexo看到很多人说hexo操作简单，速度也比较快，就打算试试。hexo是node.js写的，首先安装git和node.js，我的系统是Windows: Node.js Git 3.1 Node.js3.1.1 安装安装基本一路Next就好了，过程中会自动在环境变量的path里添加nodejs，例如“C:\Program Files\nodejs”。但需要手动添加NODE_PATH环境变量，value为node_modules，例如“C:\Program Files\nodejs\node_modules”。 3.1.2 验证打开cmd，输入： 12node -vnpm -v 3.1.3 npm配置配置npm的全局模块的存放路径以及cache的路径，在NodeJs下建立”node_global”及”node_cache”两个文件夹。打开cmd，输入： 12npm config set prefix "C:\Program Files\nodejs\node_global"npm config set cache "C:\Program Files\nodejs\node_cache" 3.2 Git安装过程就不说了，Git教程可参照Git中文教程。以下部分主要参考beiyuu的《使用Github Pages建独立博客》。 3.2.1 检查SSH keys的设置首先我们需要检查你电脑上现有的ssh key： 1$ cd ~/.ssh 如果显示“No such file or directory”，跳到第三步，否则继续。 3.2.2 备份和移除原来的ssh key设置因为已经存在key文件，所以需要备份旧的数据并删除： 12345$ lsconfig id\_rsa id\_rsa.pub known_hosts$ mkdir key_backup$ cp id\_rsa* key\_backup$ rm id_rsa* 3.2.3 生成新的SSH Key输入下面的代码，就可以生成新的key文件，我们只需要默认设置就好，所以当需要输入文件名的时候，回车就好。 123$ ssh-keygen -t rsa -C "邮件地址@youremail.com"Generating public/private rsa key pair.Enter file in which to save the key (/Users/your\_user\_directory/.ssh/id_rsa):&lt;回车就好&gt; 然后系统会要你输入加密串（Passphrase）： 12Enter passphrase (empty for no passphrase):&lt;输入加密串&gt;Enter same passphrase again:&lt;再次输入加密串&gt; 最后看到这样的界面，就成功设置ssh key了： 3.2.4 添加SSH Key到GitHub在本机设置SSH Key之后，需要添加到GitHub上，以完成SSH链接的设置。用文本编辑工具打开id_rsa.pub文件，如果看不到这个文件，你需要设置显示隐藏文件。准确的复制这个文件的内容，才能保证设置的成功。在Github主页上Settings –&gt; SSH keys –&gt; Add SSH key，把复制的内容粘贴进去，然后点击Add Key按钮即可 PS：如果需要配置多个GitHub账号，可以参看这个多个github帐号的SSH key切换，不过需要提醒一下的是，如果你只是通过这篇文章中所述配置了Host，那么你多个账号下面的提交用户会是一个人，所以需要通过命令git config –global –unset user.email删除用户账户设置，在每一个repo下面使用git config –local user.email ‘你的github邮箱@mail.com’ 命令单独设置用户账户信息 3.2.5 测试可以输入下面的命令，看看设置是否成功 1$ ssh -T git@github.com 如果是下面的反应： 123The authenticity of host 'github.com (207.97.227.239)' can't be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? 不要紧张，输入yes就好，然后会看到： 1Hi &lt;em&gt;username&lt;/em&gt;! You've successfully authenticated, but GitHub does not provide shell access. 3.3 HexoHexo 是一个简单地、轻量地、基于Node的一个静态博客框架，参考Hexo文档。利用npm命令可安装：1npm install -g hexo [--proxy http://url:port] 4. 创建hexo博客目录，新建文章并部署到github我当前Hexo的版本为3.1.1。 4.1 初始化创建hexo目录，右键选择Git bash，执行以下命令： 12hexo initnpm install 4.2 创建文章1hexo new "my new post" 会生成my new post.md文件，可以进行编辑。 4.3 生成1hexo generate 4.4 部署1hexo deploy 4.5 配置_config.yml是其配置文件，记一下踩的坑吧： 某项修改时，冒号后面格式为：一个半角空格+value，例如title: MyBlog，空格多或少都会报错 Hexo 3的deploy配置：deploy:type: gitrepository: git@github.com:username/username.github.io.gitbranch: master 另外Hexo没有删除命令，如果要删除某文章，直接到source中把文件删掉即可。 5. 绑定域名，设置DNS5.1 DNSPod使用DNSPod，注册后添加两条A记录和一条CNAME记录，其中A的两条记录指向的ip地址是github Pages的提供的ip，可在Github Pages查看。CNAME记录中记录值为username.github.io。 5.2 Godday登录后找到域名 –&gt; Manage –&gt; Nameservers，更改godaddy的Nameservers为DNSpod的NameServers f1g1ns1.dnspod.net、f1g1ns2.dnspod.net。 5.3 Github在hexo的source目录下创建CNAME文件，内容为要绑定的域名，然后hexo deploy提交到github上去。 配置完后等待域名解析生效即可。 参考：hexo系列教程：（二）搭建hexo博客Hexo搭建Github静态博客如何搭建一个独立博客——简明Github Pages与Hexo教程更换博客系统——从jekyll到hexo]]></content>
    </entry>

    
  
  
</search>
